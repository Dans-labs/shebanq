{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"left\" src=\"images/VU-ETCBC-small.png\"/></a>\n",
    "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"right\" src=\"images/laf-fabric-small.png\"/></a>\n",
    "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"right\"src=\"images/DANS-small.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complement corrections\n",
    "\n",
    "\n",
    "# 0. Introduction\n",
    "\n",
    "Joint work of Dirk Roorda and Janet Dyk.\n",
    "\n",
    "In order to do\n",
    "[flowchart analysis](https://shebanq.ancient-data.org/shebanq/static/docs/tools/valence/flowchart.html)\n",
    "on verbs, we need to correct some coding errors.\n",
    "\n",
    "Because the flowchart assigns meanings to verbs depending on the number and nature of complements found in their context, it is important that the phrases in those clauses are labeled correctly, i.e. that the\n",
    "[function](https://shebanq.ancient-data.org/shebanq/static/docs/featuredoc/features/comments/function.html)\n",
    "feature for those phrases have the correct label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "(Janet Dyk, Reinoud Oosting and Oliver Glanz, 2014) \n",
    "Analysing Valence Patterns in Biblical Hebrew: Theoretical Questions and Analytic Frameworks.\n",
    "*J. of Northwest Semitic Languages, vol. 40 (2014), no. 1, pp. 43-62*.\n",
    "[pdf abstract](http://academic.sun.ac.za/jnsl/Volumes/JNSL%2040%201%20abstracts%20and%20bookreview.pdf)\n",
    "[pdf fulltext (author's copy with deviant page numbering)](https://shebanq.ancient-data.org/static/docs/methods/2014_Dyk_jnsl.pdf)\n",
    "\n",
    "(Janet Dyk 2014)\n",
    "Deportation or Forgiveness in Hosea 1.6? Verb Valence Patterns and Translation Proposals.\n",
    "*The Bible Translator 2014, Vol. 65(3) 235–279*.\n",
    "[pdf](http://tbt.sagepub.com/content/65/3/235.full.pdf?ijkey=VK2CEHvVrvSGA5B&keytype=finite)\n",
    "\n",
    "(Janet Dyk 014)\n",
    "Traces of Valence Shift in Classical Hebrew.\n",
    "In: *Discourse, Dialogue, and Debate in the Bible: Essays in Honour of Frank Polak*.\n",
    "Ed. Athalya Brenner-Idan.\n",
    "*Sheffield Pheonix Press, 48–65*.\n",
    "[book behind pay-wall](http://www.sheffieldphoenix.com/showbook.asp?bkid=273)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Task\n",
    "In this notebook we do the following tasks:\n",
    "\n",
    "* generate correction sheets for selected verbs,\n",
    "* process the set of filled in correction sheets\n",
    "* generate sheets with computed, new features (based on corrected values, valence related) to be edited manually\n",
    "* transform the set of filled in enrichment sheets into an annotation package\n",
    "\n",
    "Between the first and second task, the sheets will have been filled in by Janet with corrections.\n",
    "Between the third and the fourth task, the sheets will be inspected and improved by Janet.\n",
    "\n",
    "The resulting annotation package offers the corrections as the value of a new feature, also called `function`, but now in the annotation space `JanetDyk` instead of `etcbc4`.\n",
    "The results of the enrichment will be added as new features in that same annotation space.\n",
    "\n",
    "## 1.1 Limitations\n",
    "We restrict ourselves to verb occurrences where the verb is the nucleus of a phrase with function *predicate*. \n",
    "There are also verb occurrences in other kinds of phrases, and these also can have complements. These cases are coded very differently in the database. See for example [Joshua 3:8](https://shebanq.ancient-data.org/hebrew/text?book=Josua&chapter=3&verse=8&version=4b&mr=m&qw=q&tp=txt_tb1&tr=hb&wget=v&qget=v&nget=v). (*and you command the priest carrying the ark* ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation\n",
    "\n",
    "Start the engines, and note the import of the `ExtraData` functionality from the `etcbc.extra` module.\n",
    "This module can turn data with anchors into additional LAF annotations to the big ETCBC LAF resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.8.3\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: https://shebanq.ancient-data.org/static/docs/featuredoc/texts/welcome.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys,os, collections\n",
    "from copy import deepcopy\n",
    "\n",
    "import laf\n",
    "from laf.fabric import LafFabric\n",
    "from etcbc.preprocess import prepare\n",
    "from etcbc.extra import ExtraData\n",
    "\n",
    "fabric = LafFabric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = 'etcbc'\n",
    "version = '4b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instruct the API to load data.\n",
    "Note that we ask for the XML identifiers, because `ExtraData` needs them to stitch the corrections into the LAF XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s DETAIL: COMPILING m: etcbc4b: UP TO DATE\n",
      "  0.01s USING main: etcbc4b DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.01s DETAIL: COMPILING a: lexicon: UP TO DATE\n",
      "  0.01s USING annox: lexicon DATA COMPILED AT: 2016-07-08T14-32-54\n",
      "  0.02s DETAIL: load main: G.node_anchor_min\n",
      "  0.07s DETAIL: load main: G.node_anchor_max\n",
      "  0.13s DETAIL: load main: G.node_sort\n",
      "  0.19s DETAIL: load main: G.node_sort_inv\n",
      "  0.66s DETAIL: load main: G.edges_from\n",
      "  0.71s DETAIL: load main: G.edges_to\n",
      "  0.77s DETAIL: load main: X. [node]  -> \n",
      "  1.87s DETAIL: load main: X. [node]  <- \n",
      "  2.53s DETAIL: load main: F.etcbc4_db_oid [node] \n",
      "  3.18s DETAIL: load main: F.etcbc4_db_otype [node] \n",
      "  3.80s DETAIL: load main: F.etcbc4_ft_function [node] \n",
      "  3.91s DETAIL: load main: F.etcbc4_ft_lex [node] \n",
      "  4.08s DETAIL: load main: F.etcbc4_ft_ls [node] \n",
      "  4.25s DETAIL: load main: F.etcbc4_ft_prs [node] \n",
      "  4.42s DETAIL: load main: F.etcbc4_ft_rela [node] \n",
      "  4.74s DETAIL: load main: F.etcbc4_ft_sp [node] \n",
      "  4.94s DETAIL: load main: F.etcbc4_ft_typ [node] \n",
      "  5.27s DETAIL: load main: F.etcbc4_ft_uvf [node] \n",
      "  5.45s DETAIL: load main: F.etcbc4_ft_vs [node] \n",
      "  5.64s DETAIL: load main: F.etcbc4_sft_chapter [node] \n",
      "  5.65s DETAIL: load main: F.etcbc4_sft_verse [node] \n",
      "  5.66s DETAIL: load main: F.etcbc4_ft_mother [e] \n",
      "  5.73s DETAIL: load main: C.etcbc4_ft_mother -> \n",
      "  5.90s DETAIL: load main: C.etcbc4_ft_mother <- \n",
      "  6.03s DETAIL: load annox lexicon: F.etcbc4_lex_nametype [node] \n",
      "  6.15s LOGFILE=/Users/dirk/laf/laf-fabric-output/etcbc4b/flow_corr/__log__flow_corr.txt\n",
      "  6.18s INFO: LOADING PREPARED data: please wait ... \n",
      "  6.18s prep prep: G.node_sort\n",
      "  6.23s prep prep: G.node_sort_inv\n",
      "  6.75s prep prep: L.node_up\n",
      "  9.62s prep prep: L.node_down\n",
      "    15s prep prep: V.verses\n",
      "    15s prep prep: V.books_la\n",
      "    15s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "    18s INFO: LOADED PREPARED data\n",
      "    18s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK flow_corr AT 2016-11-14T14-00-04\n"
     ]
    }
   ],
   "source": [
    "API = fabric.load(source+version, 'lexicon', 'flow_corr', {\n",
    "    \"xmlids\": {\"node\": True, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype\n",
    "        sp vs lex uvf prs nametype ls\n",
    "        function rela typ\n",
    "        chapter verse\n",
    "    ''','''\n",
    "        mother\n",
    "    '''),\n",
    "    \"prepare\": prepare,\n",
    "}, verbose='DETAIL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ln_base = 'https://shebanq.ancient-data.org/hebrew/text'\n",
    "ln_tpl = '?book={}&chapter={}&verse={}'\n",
    "ln_tweak = '&version=4b&mr=m&qw=n&tp=txt_tb1&tr=hb&wget=x&qget=v&nget=x'\n",
    "\n",
    "home_dir = os.path.expanduser('~').replace('\\\\', '/')\n",
    "base_dir = '{}/Dropbox/SYNVAR'.format(home_dir)\n",
    "result_dir = '{}/results'.format(base_dir)\n",
    "all_results = '{}/all.csv'.format(result_dir)\n",
    "kinds = ('corr_blank', 'corr_filled', 'enrich_blank', 'enrich_filled')\n",
    "kdir = {}\n",
    "for k in kinds:\n",
    "    kd = '{}/{}'.format(base_dir, k)\n",
    "    kdir[k] = kd\n",
    "    if not os.path.exists(kd):\n",
    "        os.makedirs(kd)\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)\n",
    "\n",
    "\n",
    "def vfile(verb, kind):\n",
    "    if kind not in kinds:\n",
    "        msg('Unknown kind `{}`'.format(kind))\n",
    "        return None\n",
    "    return '{}/{}_{}{}.csv'.format(kdir[kind], verb.replace('>','a').replace('<', 'o'), source, version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Domain\n",
    "Here is the set of verbs that interest us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verbs_initial = set('''\n",
    "    CJT\n",
    "    BR>\n",
    "    QR>\n",
    "'''.strip().split())\n",
    "\n",
    "motion_verbs = set('''\n",
    "    <BR\n",
    "    <LH\n",
    "    BW>\n",
    "    CWB\n",
    "    HLK\n",
    "    JRD\n",
    "    JY>\n",
    "    NPL\n",
    "    NWS\n",
    "    SWR\n",
    "'''.strip().split())\n",
    "\n",
    "double_object_verbs = set('''\n",
    "    NTN\n",
    "    <FH\n",
    "    FJM\n",
    "'''.strip().split())\n",
    "\n",
    "complex_qal_verbs = set('''\n",
    "    NF>\n",
    "    PQD\n",
    "'''.strip().split())\n",
    "\n",
    "verbs = verbs_initial | motion_verbs | double_object_verbs | complex_qal_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Phrase function\n",
    "\n",
    "We need to correct some values of the phrase function.\n",
    "When we receive the corrections, we check whether they have legal values.\n",
    "Here we look up the possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicate_functions = {\n",
    "    'Pred', 'PreS', 'PreO', 'PreC', 'PtcO', 'PrcS',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "legal_values = dict(\n",
    "    function={F.function.v(p) for p in F.otype.s('phrase')},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a list of occurrences of those verbs, organized by the lexeme of the verb.\n",
    "We need some extra values, to indicate other coding errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error_values = dict(\n",
    "    function=dict(\n",
    "        BoundErr='this constituent is part of another constituent and does not merit its own function/type/rela value',\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the error_values to the legal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.47s {'function': {'PreO', 'Loca', 'Ques', 'PrcS', 'Conj', 'PreC', 'NCoS', 'ModS', 'Cmpl', 'Supp', 'Voct', 'Modi', 'Subj', 'Exst', 'PtcO', 'Frnt', 'Time', 'Nega', 'PreS', 'Pred', 'Intj', 'IntS', 'PrAd', 'BoundErr', 'Adju', 'NCop', 'EPPr', 'Objc', 'Rela', 'ExsS'}}\n"
     ]
    }
   ],
   "source": [
    "for feature in set(legal_values.keys()) | set(error_values.keys()):\n",
    "    ev = error_values.get(feature, {})\n",
    "    if ev:\n",
    "        lv = legal_values.setdefault(feature, set())\n",
    "        lv |= set(ev.keys())\n",
    "inf('{}'.format(legal_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.51s Finding occurrences ...\n",
      "  3.14s Done\n",
      "Selected:     16036 verb occurrences in 15880 clauses\n",
      "  3.14s <BR   548 occurrences of which   32 outside a predicate phrase\n",
      "  3.14s <FH  2629 occurrences of which   59 outside a predicate phrase\n",
      "  3.14s <LH   890 occurrences of which   10 outside a predicate phrase\n",
      "  3.14s BR>    48 occurrences of which    3 outside a predicate phrase\n",
      "  3.14s BW>  2570 occurrences of which   27 outside a predicate phrase\n",
      "  3.14s CJT    85 occurrences of which    1 outside a predicate phrase\n",
      "  3.14s CWB  1037 occurrences of which   22 outside a predicate phrase\n",
      "  3.15s FJM   609 occurrences of which    3 outside a predicate phrase\n",
      "  3.15s HLK  1554 occurrences of which   30 outside a predicate phrase\n",
      "  3.15s JRD   377 occurrences of which   16 outside a predicate phrase\n",
      "  3.15s JY>  1069 occurrences of which   32 outside a predicate phrase\n",
      "  3.15s NF>   656 occurrences of which   52 outside a predicate phrase\n",
      "  3.15s NPL   445 occurrences of which   11 outside a predicate phrase\n",
      "  3.15s NTN  2017 occurrences of which   10 outside a predicate phrase\n",
      "  3.15s NWS   159 occurrences of which    4 outside a predicate phrase\n",
      "  3.15s PQD   303 occurrences of which   72 outside a predicate phrase\n",
      "  3.15s QR>   743 occurrences of which   11 outside a predicate phrase\n",
      "  3.15s SWR   297 occurrences of which    1 outside a predicate phrase\n"
     ]
    }
   ],
   "source": [
    "inf('Finding occurrences ...')\n",
    "# we restrict our selves to selected verbs and their contexts\n",
    "occs = collections.defaultdict(list)   # dictionary of verb occurrence nodes per verb lexeme\n",
    "npoccs = collections.defaultdict(list) # same, but those not occurring in a \"predicate\"\n",
    "clause_verb = collections.defaultdict(list)    # dictionary of verb occurrence nodes per clause node\n",
    "clause_verb_index = collections.defaultdict(set) # mapping from clauses to its main verb(s)\n",
    "verb_clause_index = collections.defaultdict(list) # mapping from verbs to the clauses of which it is main verb\n",
    "\n",
    "nw = 0\n",
    "nws = 0\n",
    "for w in F.otype.s('word'):\n",
    "    if F.sp.v(w) != 'verb': continue\n",
    "    lex = F.lex.v(w).rstrip('[')\n",
    "    if lex not in verbs: continue\n",
    "    nw += 1\n",
    "    pf = F.function.v(L.u('phrase', w))\n",
    "    if pf not in predicate_functions:\n",
    "        npoccs[lex].append(w)\n",
    "    occs[lex].append(w)\n",
    "    cn = L.u('clause', w)\n",
    "    clause_verb[cn].append(w)\n",
    "    clause_verb_index[cn].add(lex)\n",
    "    verb_clause_index[lex].append(cn)\n",
    "\n",
    "inf('Done')\n",
    "inf('Selected:    {:>6} verb occurrences in {} clauses'.format(nw, len(clause_verb)), withtime=False)\n",
    "\n",
    "for verb in sorted(verbs):\n",
    "    inf('{} {:>5} occurrences of which {:>4} outside a predicate phrase'.format(\n",
    "        verb, \n",
    "        len(occs[verb]),\n",
    "        len(npoccs[verb]),\n",
    "        withtime=False,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3 Blank sheet generation\n",
    "Generate correction sheets.\n",
    "They are CSV files. Every row corresponds to a verb occurrence.\n",
    "The fields per row are the node numbers of the clause in which the verb occurs, the node number of the verb occurrence, the text of the verb occurrence (in ETCBC transliteration, consonantal) a passage label (book, chapter, verse), and then 4 columns for each phrase in the clause:\n",
    "\n",
    "* phrase node number\n",
    "* phrase text (ETCBC translit consonantal)\n",
    "* original value of the `function` feature\n",
    "* corrected value of the `function` feature (generated as empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.70s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/BWa_etcbc4b.csv\n",
      "  3.83s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/QRa_etcbc4b.csv\n",
      "  3.91s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NPL_etcbc4b.csv\n",
      "  3.92s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/BRa_etcbc4b.csv\n",
      "  3.99s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/JRD_etcbc4b.csv\n",
      "  4.17s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/JYa_etcbc4b.csv\n",
      "  4.31s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/oLH_etcbc4b.csv\n",
      "  4.43s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NFa_etcbc4b.csv\n",
      "  4.80s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/oFH_etcbc4b.csv\n",
      "  4.82s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/CJT_etcbc4b.csv\n",
      "  5.03s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/HLK_etcbc4b.csv\n",
      "  5.38s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NTN_etcbc4b.csv\n",
      "  5.43s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/NWS_etcbc4b.csv\n",
      "  5.54s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/oBR_etcbc4b.csv\n",
      "  5.63s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/PQD_etcbc4b.csv\n",
      "  5.79s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/CWB_etcbc4b.csv\n",
      "  5.92s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/FJM_etcbc4b.csv\n",
      "  5.97s Generated correction sheet for verb /Users/dirk/Dropbox/SYNVAR/corr_blank/SWR_etcbc4b.csv\n",
      "  6.00s 51763  phrases seen 1  time(s)\n",
      "  6.01s 181    phrases seen 2  time(s)\n",
      "  6.01s 9      phrases seen 3  time(s)\n",
      "  6.01s Total phrases seen: 51953\n"
     ]
    }
   ],
   "source": [
    "phrases_seen = collections.Counter()\n",
    "\n",
    "def gen_sheet(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    field_names = '''\n",
    "        clause#\n",
    "        word#\n",
    "        passage\n",
    "        link\n",
    "        verb\n",
    "        stem\n",
    "    '''.strip().split()\n",
    "    max_phrases = 0\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cln = L.u('clause', wn)\n",
    "        if cln in clauses_seen: continue\n",
    "        clauses_seen.add(cln)\n",
    "        vn = L.u('verse', wn)\n",
    "        bn = L.u('book', wn)\n",
    "        ch = F.chapter.v(vn)\n",
    "        vs = F.verse.v(vn)\n",
    "        passage_label = T.passage(vn)\n",
    "        ln = ln_base+(ln_tpl.format(T.book_name(bn, lang='la'), ch, vs))+ln_tweak\n",
    "        lnx = '''\"=HYPERLINK(\"\"{}\"\"; \"\"link\"\")\"'''.format(ln)\n",
    "        vt = T.words([wn], fmt='ec').replace('\\n', '')\n",
    "        vstem = F.vs.v(wn)\n",
    "        np = '* ' if wn in npoccs[verb] else ''\n",
    "        row = [cln, wn, passage_label, lnx, np+vt, vstem]\n",
    "        phrases = L.d('phrase', cln)\n",
    "        n_phrases = len(phrases)\n",
    "        if n_phrases > max_phrases: max_phrases = n_phrases\n",
    "        for pn in phrases:\n",
    "            phrases_seen[pn] += 1\n",
    "            pt = T.words(L.d('word', pn), fmt='ec').replace('\\n', '')\n",
    "            pf = F.function.v(pn)\n",
    "            pnp = np if pf in predicate_functions else ''\n",
    "            row.extend((pn, pnp+pt, pf, ''))\n",
    "        rows.append(row)\n",
    "    for i in range(max_phrases):\n",
    "        field_names.extend('''\n",
    "            phr{i}#\n",
    "            phr{i}_txt\n",
    "            phr{i}_function\n",
    "            phr{i}_corr\n",
    "        '''.format(i=i+1).strip().split())\n",
    "    filename = vfile(verb, 'corr_blank')\n",
    "    row_file = open(filename, 'w')\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    inf('Generated correction sheet for verb {}'.format(filename))\n",
    "    \n",
    "for verb in verbs: gen_sheet(verb)\n",
    "    \n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    inf('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "inf('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "# 4 Processing corrections\n",
    "We read the filled-in correction sheets and extract the correction data out of it.\n",
    "We store the corrections in a dictionary keyed by the phrase node.\n",
    "We check whether we get multiple corrections for the same phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6.14s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/oBR_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6.14s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/oFH_etcbc4b.csv\n",
      "  6.18s <FH: Found   735 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/oFH_etcbc4b.csv\n",
      "  6.18s OK: Corrected phrases did not receive multiple corrections\n",
      "  6.19s OK: all corrected nodes where phrase nodes\n",
      "  6.19s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6.19s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/oLH_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6.19s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/BRa_etcbc4b.csv\n",
      "  6.19s BR>: Found   739 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/BRa_etcbc4b.csv\n",
      "  6.19s OK: Corrected phrases did not receive multiple corrections\n",
      "  6.19s OK: all corrected nodes where phrase nodes\n",
      "  6.19s OK: all corrected values are legal\n",
      "  6.19s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/BWa_etcbc4b.csv\n",
      "  6.24s BW>: Found   794 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/BWa_etcbc4b.csv\n",
      "  6.24s OK: Corrected phrases did not receive multiple corrections\n",
      "  6.24s OK: all corrected nodes where phrase nodes\n",
      "  6.24s OK: all corrected values are legal\n",
      "  6.25s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/CJT_etcbc4b.csv\n",
      "  6.25s CJT: Found   797 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/CJT_etcbc4b.csv\n",
      "  6.25s OK: Corrected phrases did not receive multiple corrections\n",
      "  6.25s OK: all corrected nodes where phrase nodes\n",
      "  6.25s OK: all corrected values are legal\n",
      "  6.25s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/CWB_etcbc4b.csv\n",
      "  6.27s CWB: Found   842 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/CWB_etcbc4b.csv\n",
      "  6.27s OK: Corrected phrases did not receive multiple corrections\n",
      "  6.28s OK: all corrected nodes where phrase nodes\n",
      "  6.28s OK: all corrected values are legal\n",
      "  6.28s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/FJM_etcbc4b.csv\n",
      "  6.30s FJM: Found   964 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/FJM_etcbc4b.csv\n",
      "  6.30s OK: Corrected phrases did not receive multiple corrections\n",
      "  6.30s OK: all corrected nodes where phrase nodes\n",
      "  6.30s OK: all corrected values are legal\n",
      "  6.30s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/HLK_etcbc4b.csv\n",
      "  6.33s HLK: Found  1123 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/HLK_etcbc4b.csv\n",
      "  6.34s OK: Corrected phrases did not receive multiple corrections\n",
      "  6.34s OK: all corrected nodes where phrase nodes\n",
      "  6.34s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6.34s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/JRD_etcbc4b.csv\n",
      "  6.35s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/JYa_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6.35s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/NFa_etcbc4b.csv\n",
      "  6.36s NF>: Found  1224 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/NFa_etcbc4b.csv\n",
      "  6.36s OK: Corrected phrases did not receive multiple corrections\n",
      "  6.36s OK: all corrected nodes where phrase nodes\n",
      "  6.36s OK: all corrected values are legal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6.37s NO file /Users/dirk/Dropbox/SYNVAR/corr_filled/NPL_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6.37s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/NTN_etcbc4b.csv\n",
      "  6.41s NTN: Found  1368 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/NTN_etcbc4b.csv\n",
      "  6.41s OK: Corrected phrases did not receive multiple corrections\n",
      "  6.41s OK: all corrected nodes where phrase nodes\n",
      "  6.42s OK: all corrected values are legal\n",
      "  6.42s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/NWS_etcbc4b.csv\n",
      "  6.43s NWS: Found  1379 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/NWS_etcbc4b.csv\n",
      "  6.43s OK: Corrected phrases did not receive multiple corrections\n",
      "  6.43s OK: all corrected nodes where phrase nodes\n",
      "  6.43s OK: all corrected values are legal\n",
      "  6.43s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/PQD_etcbc4b.csv\n",
      "  6.44s PQD: Found  1405 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/PQD_etcbc4b.csv\n",
      "  6.44s OK: Corrected phrases did not receive multiple corrections\n",
      "  6.44s OK: all corrected nodes where phrase nodes\n",
      "  6.44s OK: all corrected values are legal\n",
      "  6.44s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/QRa_etcbc4b.csv\n",
      "  6.46s QR>: Found  1408 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/QRa_etcbc4b.csv\n",
      "  6.46s OK: Corrected phrases did not receive multiple corrections\n",
      "  6.46s OK: all corrected nodes where phrase nodes\n",
      "  6.46s OK: all corrected values are legal\n",
      "  6.46s Processing /Users/dirk/Dropbox/SYNVAR/corr_filled/SWR_etcbc4b.csv\n",
      "  6.47s SWR: Found  1433 corrections in /Users/dirk/Dropbox/SYNVAR/corr_filled/SWR_etcbc4b.csv\n",
      "  6.47s OK: Corrected phrases did not receive multiple corrections\n",
      "  6.48s OK: all corrected nodes where phrase nodes\n",
      "  6.48s OK: all corrected values are legal\n",
      "  6.48s Found 1433 corrections in the phrase function\n",
      "  6.51s 41205  phrases seen 1  time(s)\n",
      "  6.51s 94     phrases seen 2  time(s)\n",
      "  6.51s 3      phrases seen 3  time(s)\n",
      "  6.51s Total phrases seen: 41302\n"
     ]
    }
   ],
   "source": [
    "phrases_seen = collections.Counter()\n",
    "pf_corr = {}\n",
    "\n",
    "def read_corr():\n",
    "    function_values = legal_values['function']\n",
    "\n",
    "    for verb in sorted(verbs):\n",
    "        repeated = collections.defaultdict(list)\n",
    "        non_phrase = set()\n",
    "        illegal_fvalue = set()\n",
    "\n",
    "        filename = vfile(verb, 'corr_filled')\n",
    "        if not os.path.exists(filename):\n",
    "            msg('NO file {}'.format(filename))\n",
    "            continue\n",
    "        else:\n",
    "            inf('Processing {}'.format(filename))\n",
    "        with open(filename) as f:\n",
    "            header = f.__next__()\n",
    "            for line in f:\n",
    "                fields = line.rstrip().split(';')\n",
    "                for i in range(1, len(fields)//4):\n",
    "                    (pn, pc) = (fields[2+4*i], fields[2+4*i+3])\n",
    "                    if pn != '':\n",
    "                        pc = pc.strip()\n",
    "                        pn = int(pn)\n",
    "                        phrases_seen[pn] += 1\n",
    "                        if pc != '':\n",
    "                            good = True\n",
    "                            for i in [1]:\n",
    "                                good = False\n",
    "                                if pn in pf_corr:\n",
    "                                    repeated[pn] += pc\n",
    "                                    continue\n",
    "                                if pc not in function_values:\n",
    "                                    illegal_fvalue.add(pc)\n",
    "                                    continue\n",
    "                                if F.otype.v(pn) != 'phrase': \n",
    "                                    non_phrase.add(pn)\n",
    "                                    continue\n",
    "                                good = True\n",
    "                            if good:\n",
    "                                pf_corr[pn] = pc\n",
    "\n",
    "        inf('{}: Found {:>5} corrections in {}'.format(verb, len(pf_corr), filename))\n",
    "        if len(repeated):\n",
    "            msg('ERROR: Some phrases have been corrected multiple times!')\n",
    "            for x in sorted(repeated):\n",
    "                msg('{:>6}: {}'.format(x, ', '.join(repeated[x])))\n",
    "        else:\n",
    "            inf('OK: Corrected phrases did not receive multiple corrections')\n",
    "        if len(non_phrase):\n",
    "            msg('ERROR: Corrections have been applied to non-phrase nodes: {}'.format(','.join(non_phrase)))\n",
    "        else:\n",
    "            inf('OK: all corrected nodes where phrase nodes')\n",
    "        if len(illegal_fvalue):\n",
    "            msg('ERROR: Some corrections supply illegal values for phrase function!')\n",
    "            msg('`{}`'.format('`, `'.join(illegal_fvalue)))\n",
    "        else:\n",
    "            inf('OK: all corrected values are legal')\n",
    "    inf('Found {} corrections in the phrase function'.format(len(pf_corr)))\n",
    "        \n",
    "read_corr()\n",
    "\n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    inf('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "inf('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. Enrichment\n",
    "\n",
    "We create blank sheets for new feature assignments, based on the corrected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6.54s 6 Enrich field specifications OK\n",
      "valence = {adjunct, complement, core}\n",
      "predication = {NA, copula, regular}\n",
      "grammatical = {*, K_object, L_object, NA, NP_direct_object, direct_object, indirect_object, infinitive_object, principal_direct_object, subject}\n",
      "original = {*, K_object, L_object, NA, NP_direct_object, direct_object, indirect_object, infinitive_object, principal_direct_object, subject}\n",
      "lexical = {location, time}\n",
      "semantic = {benefactive, instrument, location, manner, time}\n"
     ]
    }
   ],
   "source": [
    "enrich_field_spec = '''\n",
    "valence\n",
    "    adjunct\n",
    "    complement\n",
    "    core\n",
    "\n",
    "predication\n",
    "    NA\n",
    "    regular\n",
    "    copula\n",
    "\n",
    "grammatical\n",
    "    NA\n",
    "    subject\n",
    "    principal_direct_object\n",
    "    direct_object\n",
    "    NP_direct_object\n",
    "    indirect_object\n",
    "    L_object\n",
    "    K_object\n",
    "    infinitive_object\n",
    "    *\n",
    "\n",
    "original\n",
    "    NA\n",
    "    subject\n",
    "    principal_direct_object\n",
    "    direct_object\n",
    "    NP_direct_object\n",
    "    indirect_object\n",
    "    L_object\n",
    "    K_object\n",
    "    infinitive_object\n",
    "    *\n",
    "\n",
    "lexical\n",
    "    location\n",
    "    time\n",
    "\n",
    "semantic\n",
    "    benefactive\n",
    "    time\n",
    "    location\n",
    "    instrument\n",
    "    manner\n",
    "'''\n",
    "enrich_fields = collections.OrderedDict()\n",
    "cur_e = None\n",
    "for line in enrich_field_spec.strip().split('\\n'):\n",
    "    if line.startswith(' '):\n",
    "        enrich_fields.setdefault(cur_e, set()).add(line.strip())\n",
    "    else:\n",
    "        cur_e = line.strip()\n",
    "nef = len(enrich_fields)\n",
    "if None in enrich_fields:\n",
    "    msg('Invalid enrich field specification')\n",
    "else:\n",
    "    inf('{} Enrich field specifications OK'.format(nef))\n",
    "for ef in enrich_fields:\n",
    "    inf('{} = {{{}}}'.format(ef, ', '.join(sorted(enrich_fields[ef]))), withtime=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enrich_baseline_rules = dict(\n",
    "    phrase='''Adju\tAdjunct\tadjunct\tNA\tNA\t\t\t\n",
    "Cmpl\tComplement\tcomplement\tNA\t*\t\t\t\n",
    "Conj\tConjunction\tNA\tNA\tNA\t\tNA\tNA\n",
    "EPPr\tEnclitic personal pronoun\tNA\tcopula\tNA\t\t\t\n",
    "ExsS\tExistence with subject suffix\tcore\tcopula\tsubject\t\t\t\n",
    "Exst\tExistence\tcore\tcopula\tNA\t\t\t\n",
    "Frnt\tFronted element\tNA\tNA\tNA\t\tNA\tNA\n",
    "Intj\tInterjection\tNA\tNA\tNA\t\tNA\tNA\n",
    "IntS\tInterjection with subject suffix\tcore\tNA\tsubject\t\t\t\n",
    "Loca\tLocative\tadjunct\tNA\tNA\t\tlocation\tlocation\n",
    "Modi\tModifier\tNA\tNA\tNA\t\tNA\tNA\n",
    "ModS\tModifier with subject suffix\tcore\tNA\tsubject\t\t\t\n",
    "NCop\tNegative copula\tcore\tcopula\tNA\t\t\t\n",
    "NCoS\tNegative copula with subject suffix\tcore\tcopula\tsubject\t\t\t\n",
    "Nega\tNegation\tNA\tNA\tNA\t\tNA\tNA\n",
    "Objc\tObject\tcomplement\tNA\tdirect_object\t\t\t\n",
    "PrAd\tPredicative adjunct\tadjunct\tNA\tNA\t\t\t\n",
    "PrcS\tPredicate complement with subject suffix\tcore\tregular\tsubject\t\t\t\n",
    "PreC\tPredicate complement\tcore\tregular\tNA\t\t\t\n",
    "Pred\tPredicate\tcore\tregular\tNA\t\t\t\n",
    "PreO\tPredicate with object suffix\tcore\tregular\tdirect_object\t\t\t\n",
    "PreS\tPredicate with subject suffix\tcore\tregular\tsubject\t\t\t\n",
    "PtcO\tParticiple with object suffix\tcore\tregular\tdirect_object\t\t\t\n",
    "Ques\tQuestion\tNA\tNA\tNA\t\tNA\tNA\n",
    "Rela\tRelative\tNA\tNA\tNA\t\tNA\tNA\n",
    "Subj\tSubject\tcore\tNA\tsubject\t\t\t\n",
    "Supp\tSupplementary constituent\tadjunct\tNA\tNA\t\t\tbenefactive\n",
    "Time\tTime reference\tadjunct\tNA\tNA\t\ttime\ttime\n",
    "Unkn\tUnknown\tNA\tNA\tNA\t\tNA\tNA\n",
    "Voct\tVocative\tNA\tNA\tNA\t\tNA\tNA''',\n",
    "    clause='''Objc\tObject\tcomplement\tNA\tdirect_object\t\t\t\n",
    "InfC\tInfinitive Construct clause\tNA\tNA\t\t\t\t''',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6.62s Enrich baseline rules are OK (204 good)\n"
     ]
    }
   ],
   "source": [
    "transform = collections.OrderedDict((('phrase', {}), ('clause', {})))\n",
    "errors = 0\n",
    "good = 0\n",
    "\n",
    "for kind in ('phrase', 'clause'):\n",
    "    for line in enrich_baseline_rules[kind].split('\\n'):\n",
    "        x = line.split('\\t')\n",
    "        nefields = len(x) - 2\n",
    "        if len(x) - 2 != nef:\n",
    "            msg('Wrong number of fields ({} must be {}) in {}:\\n{}'.format(nefields, nef, kind, line))\n",
    "            errors += 1\n",
    "        transform[kind][x[0]] = dict(zip(enrich_fields, x[2:]))\n",
    "    for e in error_values['function']:\n",
    "        transform[kind][e] = dict(zip(enrich_fields, ['']*nef))\n",
    "\n",
    "    for f in transform[kind]:\n",
    "        for e in enrich_fields:\n",
    "            val = transform[kind][f][e]\n",
    "            if val != '' and val != 'NA' and val not in enrich_fields[e]:\n",
    "                msg('Defaults for `{}` ({}): wrong `{}` value: \"{}\"'.format(f, kind, e, val))\n",
    "                errors += 1\n",
    "            else: good += 1\n",
    "if errors:\n",
    "    msg('There were {} errors ({} good)'.format(errors, good))\n",
    "else:\n",
    "    inf('Enrich baseline rules are OK ({} good)'.format(good))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us prettyprint the baseline rules of enrichment for easier reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func    : valence        predication    grammatical    original       lexical        semantic       \n",
      "[phrase]\n",
      "Adju    : adjunct        NA             NA                                                          \n",
      "BoundErr:                                                                                           \n",
      "Cmpl    : complement     NA             *                                                           \n",
      "Conj    : NA             NA             NA                            NA             NA             \n",
      "EPPr    : NA             copula         NA                                                          \n",
      "ExsS    : core           copula         subject                                                     \n",
      "Exst    : core           copula         NA                                                          \n",
      "Frnt    : NA             NA             NA                            NA             NA             \n",
      "IntS    : core           NA             subject                                                     \n",
      "Intj    : NA             NA             NA                            NA             NA             \n",
      "Loca    : adjunct        NA             NA                            location       location       \n",
      "ModS    : core           NA             subject                                                     \n",
      "Modi    : NA             NA             NA                            NA             NA             \n",
      "NCoS    : core           copula         subject                                                     \n",
      "NCop    : core           copula         NA                                                          \n",
      "Nega    : NA             NA             NA                            NA             NA             \n",
      "Objc    : complement     NA             direct_object                                               \n",
      "PrAd    : adjunct        NA             NA                                                          \n",
      "PrcS    : core           regular        subject                                                     \n",
      "PreC    : core           regular        NA                                                          \n",
      "PreO    : core           regular        direct_object                                               \n",
      "PreS    : core           regular        subject                                                     \n",
      "Pred    : core           regular        NA                                                          \n",
      "PtcO    : core           regular        direct_object                                               \n",
      "Ques    : NA             NA             NA                            NA             NA             \n",
      "Rela    : NA             NA             NA                            NA             NA             \n",
      "Subj    : core           NA             subject                                                     \n",
      "Supp    : adjunct        NA             NA                                           benefactive    \n",
      "Time    : adjunct        NA             NA                            time           time           \n",
      "Unkn    : NA             NA             NA                            NA             NA             \n",
      "Voct    : NA             NA             NA                            NA             NA             \n",
      "[clause]\n",
      "BoundErr:                                                                                           \n",
      "InfC    : NA             NA                                                                         \n",
      "Objc    : complement     NA             direct_object                                               \n"
     ]
    }
   ],
   "source": [
    "ltpl = '{:<8}: '+('{:<15}' * nef)\n",
    "inf(ltpl.format('func', *enrich_fields), withtime=False)\n",
    "for kind in transform:\n",
    "    inf('[{}]'.format(kind), withtime=False)\n",
    "    for f in sorted(transform[kind]):\n",
    "        sfs = transform[kind][f]\n",
    "        inf(ltpl.format(f, *[sfs[sf] for sf in enrich_fields]), withtime=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Enrichment logic\n",
    "\n",
    "For certain verbs and certain conditions, we can automatically fill in some of the new features.\n",
    "For example, if the verb is `CJT`, and if an adjunct phrase is personal, starting with `L`, we know that the semantic role is *benefactive*.\n",
    "\n",
    "We will also analyse the direct and indirect objects more precisely and implement heuristics to make a distinction between complements (locative) and indirect objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the direct objects\n",
    "\n",
    "In the target clauses we will find the direct object(s).\n",
    "If there is more than one, we will compute which is the principal one.\n",
    "The others are secundary ones.\n",
    "If there is only one direct object, we do not mark it as principal.\n",
    "\n",
    "An object can be a phrase or a clause.\n",
    "\n",
    "### Clauses as objects\n",
    "We will treat clauses marked as `Objc` by feature `rela` as direct objects.\n",
    "Additionally, we identify clauses marked as `InfC` by feature `typ` as direct objects if they are preceded by the preposition *L* and if there is a direct object phrase elsewhere in the clause.\n",
    "\n",
    "We will not mark all these object clauses as principal direct objects, by rules stated later on.\n",
    "\n",
    "### Implied objects\n",
    "\n",
    "There are many cases where there is a direct object without it being marked as such in the data.\n",
    "Those are cases where there are no objective, unambiguous signals for a direct object.\n",
    "We call them *implied objects*. Examples: \n",
    "\n",
    "* the relativum in relative clauses\n",
    "* complements starting with MN (from) or L (to)\n",
    "\n",
    "In the case of implied objects we have to guess.\n",
    "Initially we assume that there are no implied objects.\n",
    "\n",
    "Later, when we inspect individual cases, we can mark principal objects and implied objects manually\n",
    "for those cases where these rules do not suffice.\n",
    "\n",
    "### Finding the principal direct object\n",
    "\n",
    "When there are multiple direct objects, we use the rules formulated by (Janet Dyk, Reinoud Oosting and Oliver Glanz, 2014) to determine which one is the principal one. The rules are stated below where we make some remarks about how we apply them to our data.\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "When looking for principal direct objects, we restrict ourselves to direct objects at the phrase level, either being complete phrases, or pronominal suffixes within phrases. The following rules express a preference for the principal direct object. In a given context, we select the direct object that is preferred by applying those rules as the principal direct object. We only apply these rules if there are at least two direct objects.\n",
    "If there is only one direct object, it is not marked as principal.\n",
    "\n",
    "#### Rule 1: pronominal suffixes > preferred above marked objects > unmarked objects\n",
    "\n",
    "In a given clause, we collect all phrases with function ``PreO`` or ``PtcO``. \n",
    "If this collection is non-empty, we pick the one that is textually first (by rule 3 below) and stop applying rules.\n",
    "Otherwise, we proceed as follows.\n",
    "\n",
    "We collect all the phrases with function ``Objc``.\n",
    "If this collection is empty, there will not be a principal object.\n",
    "Otherwise, we split it up in marked and unmarked object phrases.\n",
    "\n",
    "An object phrase is *marked* if and only if it contains, somewhere, the object marker ``>T``.\n",
    "If there are marked object phrases, we pick the one that is textually first (by rule 3 below) and stop applying rules.\n",
    "Otherwise we proceed with the next rule.\n",
    "\n",
    "#### Rule 2: determined phrases > undetermined phrases\n",
    "\n",
    "We only arrive here if there are multiple ``Objc`` phrases, neither of which is marked.\n",
    "In this case, we take the textually first one (by rule 3) which has the value ``det`` for its feature ``det``, if there is one, and stop applying rules.\n",
    "Otherwise we proceed with the next rule.\n",
    "\n",
    "#### Rule 3: earlier phrases > later phrases (by textual order)\n",
    "\n",
    "This rule is implicitly applied if one of the rules before yielded more than one candidate for the principal object. Furthermore, we arrive here if the previous rules have not selected any principal direct object, while we do have more than one ``Objc`` phrase.\n",
    "\n",
    "In this case, we pick the textually first ``Objc`` phrase.\n",
    "\n",
    "### Non principal objects\n",
    "\n",
    "In case there is a principal object, we divide the other objects into two kinds:\n",
    "* clause objects\n",
    "* phrase objects\n",
    "\n",
    "We will give the phrase objects the grammatical label `NP_direct_object`.\n",
    "\n",
    "### Complements as LK Objects\n",
    "\n",
    "In some cases, a complement functions as objects, such as in [Genesis 21:13](https://shebanq.ancient-data.org/hebrew/text?nget=v&chapter=21&book=Genesis&qw=n&tp=txt_tb1&version=4b&mr=m) *I make him (into) a people*.\n",
    "\n",
    "Candidates are those complements that: \n",
    "\n",
    "* start with either preposition ``L`` or ``K`` and\n",
    "* the ``L`` or ``K`` in question does not carry a pronominal suffix\n",
    "* should also not be followed by a body part\n",
    "\n",
    "We generated grammatical labels ``L_object`` and ``K_object`` in these cases.\n",
    "The flowchart will make a distinction between ``L_object`` and ``K_object``.\n",
    "\n",
    "An L/K object is never a *principal* direct object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "objectfuncs = set('''\n",
    "Objc PreO PtcO\n",
    "'''.strip().split())\n",
    "\n",
    "cmpl_as_obj_preps = set('''\n",
    "K L\n",
    "'''.strip().split())\n",
    "\n",
    "no_prs = set('''\n",
    "absent n/a\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "body_parts = set('''\n",
    ">NP/ >P/ >PSJM/ >YB</ >ZN/\n",
    "<JN/ <NQ/ <RP/ <YM/ <YM==/\n",
    "BHN/ BHWN/ BVN/\n",
    "CD=/ CD===/ CKM/ CN/\n",
    "DD/\n",
    "GRGRT/ GRM/ GRWN/ GW/ GW=/ GWJH/ GWPH/ GXWN/\n",
    "FPH/\n",
    "JD/ JRK/ JRKH/\n",
    "KRF/ KSL=/ KTP/\n",
    "L</ LCN/ LCWN/ LXJ/\n",
    "M<H/ MPRQT/ MTL<WT/ MTNJM/ MYX/\n",
    "NBLH=/\n",
    "P<M/ PGR/ PH/ PM/ PNH/ PT=/\n",
    "QRSL/\n",
    "R>C/ RGL/\n",
    "XDH/ XLY/ XMC=/ XRY/\n",
    "YW>R/\n",
    "ZRW</\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7.16s Finding direct objects and determining the principal one\n",
      "  7.98s Done\n",
      "  447 clauses with  1 principal  objects\n",
      "15433 clauses with  0 principal  objects\n",
      "  447 clauses with  a principal  objects\n",
      "    2 clauses with  2 direct     objects\n",
      " 6014 clauses with  1 direct     objects\n",
      " 9864 clauses with  0 direct     objects\n",
      " 6016 clauses with  a direct     objects\n",
      "  433 clauses with  1 NP         objects\n",
      "15447 clauses with  0 NP         objects\n",
      "  433 clauses with  a NP         objects\n",
      "   21 clauses with  2 L          objects\n",
      " 1041 clauses with  1 L          objects\n",
      "14818 clauses with  0 L          objects\n",
      " 1062 clauses with  a L          objects\n",
      "   62 clauses with  1 K          objects\n",
      "15818 clauses with  0 K          objects\n",
      "   62 clauses with  a K          objects\n",
      "    2 clauses with  2 clause     objects\n",
      "  180 clauses with  1 clause     objects\n",
      "15698 clauses with  0 clause     objects\n",
      "  182 clauses with  a clause     objects\n",
      "    5 clauses with  2 infinitive objects\n",
      "  342 clauses with  1 infinitive objects\n",
      "15533 clauses with  0 infinitive objects\n",
      "  347 clauses with  a infinitive objects\n",
      "15880 clauses with  a selected verb\n"
     ]
    }
   ],
   "source": [
    "inf('Finding direct objects and determining the principal one')\n",
    "clause_objects = collections.defaultdict(set)\n",
    "objects = collections.defaultdict(set)\n",
    "objects_count = collections.defaultdict(collections.Counter)\n",
    "object_kinds = (\n",
    "    'principal',\n",
    "    'direct',\n",
    "    'NP',\n",
    "    'L',\n",
    "    'K',\n",
    "    'clause',\n",
    "    'infinitive',\n",
    ")\n",
    "\n",
    "def is_marked(phr):\n",
    "    # simple criterion for determining whether a direct object is marked:\n",
    "    # has it the object marker somewhere?\n",
    "    words = L.d('word', p)\n",
    "    has_et = False\n",
    "    for w in words:\n",
    "        if F.lex.v(w) == '>T':\n",
    "            has_et = True\n",
    "            break\n",
    "    return has_et\n",
    "\n",
    "for c in clause_verb:\n",
    "    these_objects = collections.defaultdict(set)\n",
    "    direct_objects_cat = collections.defaultdict(set)\n",
    "\n",
    "    for p in L.d('phrase', c):\n",
    "        pf = pf_corr.get(p, F.function.v(p))  # NB we take the corrected value for phrase function if there is one\n",
    "        if pf in objectfuncs:\n",
    "            direct_objects_cat['p_'+pf].add(p)\n",
    "            these_objects['direct'].add(p)\n",
    "        elif pf == 'Cmpl':\n",
    "            pwords = L.d('word', p)\n",
    "            w1 = pwords[0]\n",
    "            w1l = F.lex.v(w1)\n",
    "            w2l = F.lex.v(pwords[1]) if len(pwords) > 1 else None\n",
    "            if w1l in cmpl_as_obj_preps and F.prs.v(w1) in no_prs and not (w1l == 'L' and w2l in body_parts):\n",
    "                if w1l == 'K': these_objects['K'].add(p)\n",
    "                elif w1l == 'L': these_objects['L'].add(p)\n",
    "        \n",
    "    # find clause objects\n",
    "    for ac in L.d('clause', L.u('sentence', c)):\n",
    "        mothers = list(C.mother.v(ac))\n",
    "        if not (mothers and mothers[0] == c): continue\n",
    "        cr = F.rela.v(ac)\n",
    "        ct = F.typ.v(ac)\n",
    "        if cr in {'Objc'} or ct in {'InfC'}:\n",
    "            clause_objects[c].add(ac)\n",
    "            if cr in {'Objc'}:\n",
    "                label = cr\n",
    "                direct_objects_cat['c_'+label].add(ac)\n",
    "                these_objects['direct'].add(ac)\n",
    "                these_objects['clause'].add(ac)\n",
    "            elif ct in {'InfC'}:\n",
    "                if F.lex.v(L.d('word', ac)[0]) == 'L':\n",
    "                    these_objects['infinitive'].add(ac)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # order the objects in the natural ordering\n",
    "    direct_objects_order = sorted(these_objects.get('direct', set()), key=NK)\n",
    "    nobjects = len(direct_objects_order)\n",
    "\n",
    "    # compute the principal object\n",
    "    principal_object = None\n",
    "\n",
    "    for x in [1]:\n",
    "        # just one object \n",
    "        if nobjects == 1:\n",
    "            # we have chosen not to mark a principal object if there is only one object\n",
    "            # the alternative is to mark it if it is a phrase. Uncomment the next 2 lines if you want this\n",
    "            # theobject = list(dobjects_set)[0]\n",
    "            # if F.otype.v(theobject) == 'phrase': principal_object = theobject\n",
    "            break\n",
    "        # rule 1: suffixes and promoted objects\n",
    "        principal_candidates =\\\n",
    "            direct_objects_cat.get('p_PreO', set()) |\\\n",
    "            direct_objects_cat.get('p_PtcO', set())\n",
    "        if len(principal_candidates) != 0:\n",
    "            principal_object = sorted(principal_candidates, key=NK)[0]\n",
    "            break\n",
    "        principal_candidates = direct_objects_cat.get('p_Objc', set())\n",
    "        if len(principal_candidates) != 0:\n",
    "            if len(principal_candidates) == 1:\n",
    "                principal_object = list(principal_candidates)[0]\n",
    "                break\n",
    "            objects_marked = set()\n",
    "            objects_unmarked = set()\n",
    "            for p in principal_candidates:\n",
    "                if is_marked(p):\n",
    "                    objects_marked.add(p)\n",
    "                else:\n",
    "                    objects_unmarked.add(p)\n",
    "            if len(objects_marked) != 0:\n",
    "                principal_object = sorted(objects_marked, key=NK)[0]\n",
    "                break\n",
    "            if len(objects_unmarked) != 0:\n",
    "                principal_object = sorted(objects_unmarked, key=NK)[0]\n",
    "                break            \n",
    "    if principal_object != None:\n",
    "        these_objects['principal'].add(principal_object)\n",
    "    if len(these_objects['infinitive']) and not len(these_objects['direct']):\n",
    "        # we do not mark an infinitive object if there is no proper direct object around\n",
    "        these_objects['infinitive'] = set()\n",
    "    if len(these_objects['principal']):\n",
    "        these_objects['direct'] -= these_objects['principal']\n",
    "        for x in these_objects['direct'] - these_objects['clause']:\n",
    "            # the NP objects are the non-principal phrase like direct objects\n",
    "            these_objects['NP'].add(x)\n",
    "        these_objects['direct'] -= these_objects['NP']\n",
    "\n",
    "    for kind in object_kinds:\n",
    "        n = len(these_objects.get(kind, set()))\n",
    "        objects_count[kind][n] += 1\n",
    "        if n:\n",
    "            objects[kind] |= these_objects[kind]\n",
    "\n",
    "inf('Done')\n",
    "\n",
    "for kind in object_kinds:\n",
    "    total = 0\n",
    "    for (count, n) in sorted(objects_count[kind].items(), key=lambda y: -y[0]):\n",
    "        if count: total += n\n",
    "        inf('{:>5} clauses with {:>2} {:<10} objects'.format(n, count, kind), withtime=False)\n",
    "    inf('{:>5} clauses with {:>2} {:<10} objects'.format(total, 'a', kind), withtime=False)\n",
    "inf('{:>5} clauses with {:>2} selected verb'.format(len(clause_verb), 'a'), withtime=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding indirect objects\n",
    "\n",
    "The ETCBC database has not feature that marks indirect objects.\n",
    "We will use computation to determine whether a complement is an indirect object or a locative.\n",
    "This computation is just an approximation.\n",
    "\n",
    "#### Cues for a locative complement\n",
    "\n",
    "* ``# loc lexemes`` how many distinct lexemes with a locative meaning occur in the complement (given by a fixed list)\n",
    "* ``# topo`` how many lexemes with nametype = ``topo`` occur in the complement (nametype is a feature of the lexicon)\n",
    "* ``# prep_b`` how many occurrences of the preposition ``B`` occur in the complement\n",
    "* ``# h_loc`` how many H-locales are carried on words in the complement\n",
    "* ``body_part`` is 2 if the phrase starts with the preposition ``L`` followed by a body part, else 0\n",
    "* ``locativity`` ($loc$) a crude measure of the locativity of the complement, just the sum of ``# loc lexemes``, ``#topo``, ``# prep_b``, ``# h_loc`` and ``body_part``.\n",
    "\n",
    "#### Cues for an indirect object\n",
    "* ``# prep_l`` how many occurrences of the preposition ``L`` or ``>L`` with a pronominal suffix on it occur in the complement\n",
    "* ``# L prop`` how many occurrences of ``L`` or ``>L`` plus proper name or person reference word occur in the complement\n",
    "* ``indirect object`` ($ind$) a crude indicator of whether the complement is an indirect object, just the sum of ``# prep_l`` and ``# L prop`` \n",
    "\n",
    "#### The decision\n",
    "\n",
    "We take a decision as follows.\n",
    "The outcome is $L$ (complement is *locative*) or $I$ (complement is *indirect object*) or $C$ (complement is neither *locative* nor *indirect object*)\n",
    "\n",
    "(1) $ loc > 0 \\wedge ind = 0 \\Rightarrow L $\n",
    "\n",
    "(2) $ loc = 0 \\wedge ind > 0 \\Rightarrow I $\n",
    "\n",
    "(3) $ loc > 0 \\wedge ind > 0 \\wedge\\ loc - 1 > ind \\Rightarrow L$\n",
    "\n",
    "(4) $ loc > 0 \\wedge ind > 0 \\wedge\\ loc + 1 < ind \\Rightarrow I$\n",
    "\n",
    "(5) $ loc > 0 \\wedge ind > 0 \\wedge |ind - loc| <= 1 \\Rightarrow C$\n",
    "\n",
    "In words:\n",
    "\n",
    "* if there are positive signals for L or I and none for the other, we choose the one for which there are positive signals;\n",
    "* if there are positive signals for both L and I, we follow the majority count, but only if the difference is at least two;\n",
    "* in all other cases we leave it at C: not necessarilty locative and not necessarily indirect object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complfuncs = set('''\n",
    "Cmpl PreC\n",
    "'''.strip().split())\n",
    "\n",
    "cmpl_as_iobj_preps = set('''\n",
    "L >L\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locative_lexemes = set('''\n",
    ">RY/ >YL/ >XR/\n",
    "<BR/ <BRH/ <BWR/ <C==/ <JR/ <L=/ <LJ=/ <LJH/ <LJL/ <MD=/ <MDH/ <MH/ <MQ/ <MQ===/ <QB/\n",
    "BJT/\n",
    "CM CMJM/ CMC/ C<R/\n",
    "DRK/\n",
    "FDH/\n",
    "HR/\n",
    "JM/ JRDN/ JRWCLM/ JFR>L/\n",
    "MDBR/ MW<D/ MWL/ MZBX/ MYRJM/ MQWM/ MR>CWT/ MSB/ MSBH/ MVH==/\n",
    "QDM/\n",
    "SBJB/\n",
    "TJMN/ TXT/ TXWT/\n",
    "YPWN/\n",
    "'''.strip().split())\n",
    "\n",
    "personal_lexemes = set('''\n",
    ">B/ >CH/ >DM/ >DRGZR/ >DWN/ >JC/ >J=/ >KR/ >LJL/ >LMN=/ >LMNH/ >LMNJ/ >LWH/ >LWP/ >M/ \n",
    ">MH/ >MN==/ >MWN=/ >NC/ >NWC/ >PH/ >PRX/ >SJR/ >SJR=/ >SP/ >X/ >XCDRPN/\n",
    ">XWH/ >XWT/\n",
    "<BDH=/ <CWQ/ <D=/ <DH=/ <LMH/ <LWMJM/ <M/ <MD/ <MJT/ <QR=/ <R/ <WJL/ <WL/ <WL==/ <WLL/\n",
    "<WLL=/ <YRH/\n",
    "B<L/ B<LH/ BKJRH/ BKR/ BN/ BR/ BR===/ BT/ BTWLH/ BWQR/ BXRJM/ BXWN/ BXWR/\n",
    "CD==/ CDH/ CGL/ CKN/ CLCJM/ CLJC=/ CMRH=/ CPXH/ CW<R/ CWRR/\n",
    "DJG/ DWD/ DWDH/ DWG/ DWR/\n",
    "F<JR=/ FB/ FHD/ FR/ FRH/ FRJD/ FVN/\n",
    "GBJRH/ GBR/ GBR=/ GBRT/ GLB/ GNB/ GR/ GW==/ GWJ/ GZBR/\n",
    "HDBR/ \n",
    "J<RH/ JBM/ JBMH/ JD<NJ/ JDDWT/ JLD/ JLDH/ JLJD/ JRJB/ JSWR/ JTWM/ JWYR/\n",
    "JYRJM/ \n",
    "KCP=/ KHN/ KLH/ KMR/ KN<NJ=/ KNT/ KRM=/ KRWB/ KRWZ/\n",
    "L>M/ LHQH/ LMD/ LXNH/\n",
    "M<RMJM/ M>WRH/ MCBR/ MCJX/ MCM<T/ MCMR/ MCPXH/ MCQLT/ MD<=/ MD<T/ MG/\n",
    "MJNQT/ MKR=/ ML>K/ MLK/ MLKH/ MLKT/ MLX=/ MLYR/ MMZR/ MNZRJM/ MPLYT/\n",
    "MPY=/ MQHL/ MQY<H/ MR</ MR>/ MSGR=/ MT/ MWRH/ MYBH=/\n",
    "N<R/ N<R=/ N<RH/ N<RWT/ N<WRJM/ NBJ>/ NBJ>H/ NCJN/ NFJ>/ NGJD/ NJN/ NKD/ \n",
    "NKR/ NPC/ NPJLJM/ NQD/ NSJK/ NTJN/ \n",
    "PLGC/ PLJL/ PLJV/ PLJV=/ PQJD/ PR<H/ PRC/ PRJY/ PRJY=/ PRTMJM/ PRZWN/ \n",
    "PSJL/ PSL/ PVR/ PVRH/ PXH/ PXR/\n",
    "QBYH/ QCRJM/ QCT=/ QHL/ QHLH/ QHLT/ QJM/ QYJN/\n",
    "R<H=/ R<H==/ R<JH/ R<=/ R<WT/ R>H/ RB</ RB=/ RB==/ RBRBNJN/ RGMH/ RHB/ RKB=/\n",
    "RKJL/ RMH/ RQX==/ \n",
    "SBL/ SPR=/ SRJS/ SRK/ SRNJM/ \n",
    "T<RWBWT/ TLMJD/ TLT=/ TPTJ/ TR<=/ TRCT>/ TRTN/ TWCB/ TWL<H/ TWLDWT/ TWTX/\n",
    "VBX/ VBX=/ VBXH=/ VPSR/ VPXJM/\n",
    "WLD/\n",
    "XBL==/ XBL======/ XBR/ XBR=/ XBR==/ XBRH/ XBRT=/ XJ=/ XLC/ XM=/ XMWT/\n",
    "XMWY=/ XNJK/ XR=/ XRC/ XRC====/ XRP=/ XRVM/ XTN/ XTP/ XZH=/\n",
    "Y<JRH/ Y>Y>JM/ YJ/ YJD==/ YJR==/ YR=/ YRH=/ \n",
    "ZKWR/ ZMR=/ ZR</\n",
    "'''.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8.18s Determinig kind of complements\n",
      "  8.68s Done\n",
      "Phrases of kind L :   4448\n",
      "Phrases of kind C :   4219\n",
      "Phrases of kind I :   1709\n",
      "Total complements :  10376\n",
      "Total phrases     :  51953\n"
     ]
    }
   ],
   "source": [
    "inf('Determinig kind of complements')\n",
    "\n",
    "complements_c = collections.defaultdict(lambda: collections.defaultdict(lambda: []))\n",
    "complements = {}\n",
    "complementk = {}\n",
    "kcomplements = collections.Counter()\n",
    "\n",
    "nphrases = 0\n",
    "ncomplements = 0\n",
    "\n",
    "for c in clause_verb:\n",
    "    for p in L.d('phrase', c):\n",
    "        nphrases += 1\n",
    "        pf = pf_corr.get(p, F.function.v(p))\n",
    "        if pf not in complfuncs: continue\n",
    "        ncomplements += 1\n",
    "        words = L.d('word', p)\n",
    "        lexemes = [F.lex.v(w) for w in words]\n",
    "        lexeme_set = set(lexemes)\n",
    "\n",
    "        # measuring locativity\n",
    "        lex_locativity = len(locative_lexemes & lexeme_set)\n",
    "        prep_b = len([x for x in lexeme_set if x == 'B'])\n",
    "        topo = len([x for x in words if F.nametype.v(x) == 'topo'])\n",
    "        h_loc = len([x for x in words if F.uvf.v(x) == 'H'])\n",
    "        body_part = 0\n",
    "        if len(words) > 1 and F.lex.v(words[0]) == 'L' and F.lex.v(words[1]) in body_parts:\n",
    "            body_part = 2\n",
    "        loca = lex_locativity + topo + prep_b + h_loc + body_part\n",
    "\n",
    "        # measuring indirect object\n",
    "        prep_l = len([x for x in words if F.lex.v(x) in cmpl_as_iobj_preps and F.prs.v(x) not in no_prs])\n",
    "        prep_lpr = 0\n",
    "        lwn = len(words)\n",
    "        for (n, wn) in enumerate(words):\n",
    "            if F.lex.v(wn) in cmpl_as_iobj_preps:\n",
    "                if n+1 < lwn:\n",
    "                    nextw = words[n+1]\n",
    "                    if F.lex.v(nextw) in personal_lexemes or F.ls.v(nextw) == 'gntl' or (\n",
    "                        F.sp.v(nextw) == 'nmpr' and F.nametype.v(nextw) == 'pers'):\n",
    "                        prep_lpr += 1                        \n",
    "        indi = prep_l + prep_lpr\n",
    "\n",
    "        # the verdict\n",
    "        ckind = 'C'\n",
    "        if loca == 0 and indi > 0: ckind = 'I'\n",
    "        elif loca > 0 and indi == 0: ckind = 'L'\n",
    "        elif loca > indi + 1: ckind = 'L'\n",
    "        elif loca < indi - 1: ckind = 'I'\n",
    "        complementk[p] = (loca, indi, ckind)\n",
    "        kcomplements[ckind] += 1\n",
    "        complements_c[c][ckind].append(p)\n",
    "        complements[p] = (pf, ckind)\n",
    "\n",
    "inf('Done')\n",
    "for (label, n) in sorted(kcomplements.items(), key=lambda y: -y[1]):\n",
    "    inf('Phrases of kind {:<2}: {:>6}'.format(label, n), withtime=False)\n",
    "inf('Total complements : {:>6}'.format(ncomplements), withtime=False)\n",
    "inf('Total phrases     : {:>6}'.format(nphrases), withtime=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def has_L(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len(words) > 0 and F.lex.v(words[0] == 'L')\n",
    "\n",
    "def is_lex_personal(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len(words) > 1 and F.lex.v(words[1] in personal_lexemes)\n",
    "\n",
    "def is_lex_local(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len({F.lex.v(w) for w in words} & locative_lexemes) > 0\n",
    "\n",
    "def has_H_locale(vl, pn):\n",
    "    words = L.d('word', pn)\n",
    "    return len({w for w in words if F.uvf.v(w) == 'H'}) > 0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic logic\n",
    "\n",
    "This is the function that applies the generic rules about (in)direct objects and locatives.\n",
    "It takes a phrase node and a set of new label values, and modifies those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grule_as_str = {\n",
    "    'pdos':   '''direct_object => principal_direct_object''',\n",
    "    'pdos-x': '''non-object => principal_direct_object''',\n",
    "    'ndos':   '''direct_object => NP_direct_object''',\n",
    "    'ndos-x': '''non-object => NP_direct_object''',\n",
    "    'dos':    '''non-object => direct_object''',\n",
    "    'ldos':   '''non-object => L_object''',\n",
    "    'kdos':   '''non-object => K_object''',\n",
    "    'inds-c': '''complement => indirect_object''',\n",
    "    'locs-c': '''complement => location''',\n",
    "    'inds-p': '''predicate complement => indirect_object''',\n",
    "    'locs-p': '''predicate complement => location''',\n",
    "    'cdos':   '''direct-object =(superfluously)=> direct object (clause)''',\n",
    "    'cdos-x': '''non-object => direct object (clause)''',\n",
    "    'idos':   '''infinitive_object =(superfluously)=> infinitive_object (clause)''',\n",
    "    'idos-x': '''infinitive clause => infinitive_object''',\n",
    "}\n",
    "\n",
    "def rule_as_str_g(x, i): return '{}-{}'.format(i, grule_as_str[i])\n",
    "\n",
    "rule_as_str = dict(\n",
    "    generic=rule_as_str_g,\n",
    ")\n",
    "\n",
    "def generic_logic_p(pn, values):\n",
    "    gl = None\n",
    "    if pn in objects['principal']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv == 'direct_object':\n",
    "            gl = 'pdos'\n",
    "        else:\n",
    "            gl = 'pdos-x'\n",
    "            values['original'] = oldv\n",
    "        values['grammatical'] = 'principal_direct_object'\n",
    "    elif pn in objects['NP']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv == 'direct_object':\n",
    "            gl = 'ndos'\n",
    "        else:\n",
    "            gl = 'ndos-x'\n",
    "            values['original'] = oldv\n",
    "        values['grammatical'] = 'NP_direct_object'\n",
    "    elif pn in objects['direct']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv != 'direct_object':\n",
    "            gl = 'dos'\n",
    "            values['original'] = oldv\n",
    "            values['grammatical'] = 'direct_object'\n",
    "    elif pn in objects['L']:\n",
    "        oldv = values['grammatical']\n",
    "        gl = 'ldos'\n",
    "        values['original'] = oldv\n",
    "        values['grammatical'] = 'L_object'\n",
    "    elif pn in objects['K']:\n",
    "        oldv = values['grammatical']\n",
    "        gl = 'kdos'\n",
    "        values['original'] = oldv\n",
    "        values['grammatical'] = 'K_object'\n",
    "    elif pn in complements:\n",
    "        (pf, ck) = complements[pn]\n",
    "        if ck in {'I', 'L'}:\n",
    "            if pf == 'Cmpl':\n",
    "                if ck == 'I':\n",
    "                    values['grammatical'] = 'indirect_object'\n",
    "                    gl = 'inds-c'\n",
    "                else:\n",
    "                    values['valence'] = 'adjunct'\n",
    "                    values['lexical'] = 'location'\n",
    "                    values['semantic'] = 'location'\n",
    "                    gl = 'locs-c'\n",
    "            elif pf == 'PreC':\n",
    "                if ck == 'I':\n",
    "                    values['grammatical'] = 'indirect_object'\n",
    "                    gl = 'inds-p'\n",
    "                else:\n",
    "                    values['lexical'] = 'location'\n",
    "                    values['semantic'] = 'location'\n",
    "                    gl = 'locs-p'\n",
    "    return gl\n",
    "\n",
    "def generic_logic_c(cn, values):\n",
    "    gl = None\n",
    "    if cn in objects['clause']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv == 'direct_object':\n",
    "            gl = 'cdos'\n",
    "        else:\n",
    "            gl = 'cdos-x'\n",
    "            values['original'] = oldv\n",
    "        values['grammatical'] = 'direct_object'\n",
    "    elif cn in objects['infinitive']:\n",
    "        oldv = values['grammatical']\n",
    "        if oldv == 'infinitive_object':\n",
    "            gl = 'idos'\n",
    "        else:\n",
    "            gl = 'idos-x'\n",
    "            values['original'] = oldv\n",
    "        values['grammatical'] = 'infinitive_object'\n",
    "    return gl\n",
    "\n",
    "generic_logic = dict(\n",
    "    phrase=generic_logic_p,\n",
    "    clause=generic_logic_c,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Verb specific rules\n",
    "\n",
    "The verb-specific enrichment rules are stored in a dictionary, keyed  by the verb lexeme.\n",
    "The rule itself is a list of items.\n",
    "\n",
    "The last item is a tuple of conditions that need to be fulfilled to apply the rule.\n",
    "\n",
    "A condition can take the shape of\n",
    "\n",
    "* a function, taking a phrase or clause node as argument and returning a boolean value\n",
    "* an ETCBC feature for phrases or clauses : value, \n",
    "  which is true iff that feature has that value for the phrase or clause in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enrich_logic = dict(\n",
    "    phrase={\n",
    "        'CJT': [\n",
    "            (\n",
    "                ('semantic', 'benefactive'), \n",
    "                ('function:Adju', has_L, is_lex_personal),\n",
    "            ),\n",
    "            (\n",
    "                ('lexical', 'location'),\n",
    "                ('function:Cmpl', has_H_locale),\n",
    "            ),\n",
    "            (\n",
    "                ('lexical', 'location'),\n",
    "                ('semantic', 'location'),\n",
    "                ('function:Cmpl', is_lex_local),\n",
    "            ),\n",
    "        ],\n",
    "    },\n",
    "    clause={\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CJT-1\n",
      "\tIF   function   = Adju    \n",
      "\tAND  has_L          \n",
      "\tAND  is_lex_personal\n",
      "\tTHEN\n",
      "\t\tsemantic   => benefactive    \n",
      "\n",
      "CJT-2\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  has_H_locale   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\n",
      "CJT-3\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  is_lex_local   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\t\tsemantic   => location       \n",
      "\n",
      "All 3 rules OK\n"
     ]
    }
   ],
   "source": [
    "rule_index = collections.defaultdict(lambda: [])\n",
    "\n",
    "def rule_as_str_s(vl, i):\n",
    "    (conditions, sfassignments) = rule_index[vl][i]\n",
    "    label = '{}-{}\\n'.format(vl, i+1)\n",
    "    rule = '\\tIF   {}'.format('\\n\\tAND  '.join(\n",
    "        '{:<10} = {:<8}'.format(\n",
    "                *c.split(':')\n",
    "            ) if type(c) is str else '{:<15}'.format(\n",
    "                c.__name__\n",
    "            ) for c in conditions,\n",
    "    ))\n",
    "    ass = []\n",
    "    for (i, sfa) in enumerate(sfassignments):\n",
    "        ass.append('\\t\\t{:<10} => {:<15}\\n'.format(*sfa))\n",
    "    return '{}{}\\n\\tTHEN\\n{}'.format(label, rule, ''.join(ass))\n",
    "\n",
    "rule_as_str['specific'] = rule_as_str_s\n",
    "\n",
    "def check_logic():\n",
    "    errors = 0\n",
    "    nrules = 0\n",
    "    for kind in sorted(enrich_logic):\n",
    "        for vl in sorted(enrich_logic[kind]):\n",
    "            for items in enrich_logic[kind][vl]:\n",
    "                rule_index[vl].append((items[-1], items[0:-1]))\n",
    "            for (i, (conditions, sfassignments)) in enumerate(rule_index[vl]):\n",
    "                inf(rule_as_str_s(vl, i), withtime=False)\n",
    "                nrules += 1\n",
    "                for (sf, sfval) in sfassignments:\n",
    "                    if sf not in enrich_fields:\n",
    "                        msg('{}: \"{}\" not a valid enrich field'.format(kind, sf), withtime=False)\n",
    "                        errors += 1\n",
    "                    elif sfval not in enrich_fields[sf]:\n",
    "                        msg('{}: `{}`: \"{}\" not a valid enrich field value'.format(kind, sf, sfval), withtime=False)\n",
    "                        errors += 1\n",
    "                for c in conditions:\n",
    "                    if type(c) == str:\n",
    "                        x = c.split(':')\n",
    "                        if len(x) != 2:\n",
    "                            msg('{}: Wrong feature condition {}'.format(kind, c), withtime=False)\n",
    "                            errors += 1\n",
    "                        else:\n",
    "                            (feat, val) = x\n",
    "                            if feat not in legal_values:\n",
    "                                msg('{}: Feature `{}` not in use'.format(kind, feat), withtime=False)\n",
    "                                errors += 1\n",
    "                            elif val not in legal_values[feat]:\n",
    "                                msg('{}: Feature `{}`: not a valid value \"{}\"'.format(kind, feat, val), withtime=False)\n",
    "                                errors += 1\n",
    "    if errors:\n",
    "        msg('There were {} errors in {} rules'.format(errors, nrules), withtime=False)\n",
    "    else:\n",
    "        inf('All {} rules OK'.format(nrules), withtime=False)\n",
    "\n",
    "check_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rule_cases = collections.defaultdict(lambda: collections.defaultdict(lambda: {}))\n",
    "\n",
    "def apply_logic(kind, vl, n, init_values):\n",
    "    values = deepcopy(init_values)\n",
    "    gr = generic_logic[kind](n, values)\n",
    "    if gr:\n",
    "        rule_cases['generic'][kind].setdefault(('', gr), []).append(n)\n",
    "    verb_rules = enrich_logic[kind].get(vl, [])\n",
    "    for (i, items) in enumerate(verb_rules):\n",
    "        conditions = items[-1]\n",
    "        sfassignments = items[0:-1]\n",
    "\n",
    "        ok = True\n",
    "        for condition in conditions:\n",
    "            if type(condition) is str:\n",
    "                (feature, value) = condition.split(':')\n",
    "                if feature == 'function' and kind == 'phrase':\n",
    "                    fval = pf_corr.get(n, F.function.v(n))\n",
    "                else:\n",
    "                    fval = F.item[feature].v(n)\n",
    "                this_ok =  fval == value\n",
    "            else:\n",
    "                this_ok = condition(vl, n)\n",
    "            if not this_ok:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            for (sf, sfval) in sfassignments:\n",
    "                values[sf] = sfval\n",
    "            rule_cases['specific'][kind].setdefault((vl, i), []).append(n)\n",
    "    return tuple(values[sf] for sf in enrich_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnode#\n",
      "vnode#\n",
      "onode#\n",
      "book\n",
      "chapter\n",
      "verse\n",
      "verb_lexeme\n",
      "verb_stem\n",
      "verb_occurrence\n",
      "text\n",
      "constituent\n",
      "type\n",
      "function\n",
      "type\n",
      "rela\n",
      "valence\n",
      "predication\n",
      "grammatical\n",
      "original\n",
      "lexical\n",
      "semantic\n"
     ]
    }
   ],
   "source": [
    "COMMON_FIELDS = '''\n",
    "    cnode#\n",
    "    vnode#\n",
    "    onode#\n",
    "    book\n",
    "    chapter\n",
    "    verse\n",
    "    verb_lexeme\n",
    "    verb_stem\n",
    "    verb_occurrence\n",
    "    text\n",
    "    constituent\n",
    "'''.strip().split()\n",
    "\n",
    "PHRASE_FIELDS = '''\n",
    "    type\n",
    "    function\n",
    "'''.strip().split()\n",
    "\n",
    "CLAUSE_FIELDS = '''\n",
    "    type\n",
    "    rela\n",
    "'''.strip().split()\n",
    "\n",
    "field_names = COMMON_FIELDS + PHRASE_FIELDS + CLAUSE_FIELDS + list(enrich_fields) \n",
    "pfillrows = len(CLAUSE_FIELDS)\n",
    "cfillrows = len(PHRASE_FIELDS)\n",
    "fillrows =  pfillrows + cfillrows\n",
    "inf('\\n'.join(field_names), withtime=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated enrichment sheet for verb BW> (11102 rows)\n",
      "Generated enrichment sheet for verb QR> ( 3332 rows)\n",
      "Generated enrichment sheet for verb NPL ( 1932 rows)\n",
      "Generated enrichment sheet for verb BR> (  198 rows)\n",
      "Generated enrichment sheet for verb JRD ( 1593 rows)\n",
      "Generated enrichment sheet for verb JY> ( 4630 rows)\n",
      "Generated enrichment sheet for verb <LH ( 3893 rows)\n",
      "Generated enrichment sheet for verb NF> ( 2875 rows)\n",
      "Generated enrichment sheet for verb <FH (11323 rows)\n",
      "Generated enrichment sheet for verb CJT (  381 rows)\n",
      "Generated enrichment sheet for verb HLK ( 5819 rows)\n",
      "Generated enrichment sheet for verb NTN ( 9846 rows)\n",
      "Generated enrichment sheet for verb NWS (  617 rows)\n",
      "Generated enrichment sheet for verb <BR ( 2320 rows)\n",
      "Generated enrichment sheet for verb PQD ( 1283 rows)\n",
      "Generated enrichment sheet for verb CWB ( 4247 rows)\n",
      "Generated enrichment sheet for verb FJM ( 2915 rows)\n",
      "Generated enrichment sheet for verb SWR ( 1281 rows)\n",
      "    15s Done\n",
      "specific-phrase rules:\n",
      "   1 x\n",
      "\tCJT-2\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  has_H_locale   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\n",
      "\t731940\n",
      "\n",
      "   9 x\n",
      "\tCJT-3\n",
      "\tIF   function   = Cmpl    \n",
      "\tAND  is_lex_local   \n",
      "\tTHEN\n",
      "\t\tlexical    => location       \n",
      "\t\tsemantic   => location       \n",
      "\n",
      "\t606396, 619338, 630956, 654145, 731940, 776266, 789542, 794185, 797377\n",
      "\n",
      "   5 x\n",
      "\tCJT-1\n",
      "\tIF   function   = Adju    \n",
      "\tAND  has_L          \n",
      "\tAND  is_lex_personal\n",
      "\tTHEN\n",
      "\t\tsemantic   => benefactive    \n",
      "\n",
      "\t615130, 630648, 712015, 794512, 797440\n",
      "\n",
      "    15 specific-phrase rule applications\n",
      "    15 specific rule applications\n",
      "generic-clause rules:\n",
      " 184 x\n",
      "\tcdos-direct-object =(superfluously)=> direct object (clause)\n",
      "\t428049, 436377, 502784, 509042, 513973, 431301, 438165, 449836, 469303, 472341\n",
      "\n",
      " 353 x\n",
      "\tidos-x-infinitive clause => infinitive_object\n",
      "\t427133, 427141, 428639, 431036, 431406, 435223, 436526, 436613, 437746, 438669\n",
      "\n",
      "   537 generic-clause rule applications\n",
      "generic-phrase rules:\n",
      "1374 x\n",
      "\tinds-c-complement => indirect_object\n",
      "\t606715, 606885, 606980, 607018, 607211, 607224, 608957, 609021, 609045, 609736\n",
      "\n",
      "1095 x\n",
      "\tldos-non-object => L_object\n",
      "\t606137, 611592, 613369, 613419, 613636, 623449, 634755, 634816, 634825, 636629\n",
      "\n",
      "   4 x\n",
      "\tinds-p-predicate complement => indirect_object\n",
      "\t853847, 704297, 691074, 654844\n",
      "\n",
      "  62 x\n",
      "\tkdos-non-object => K_object\n",
      "\t606308, 823396, 616868, 736286, 737544, 783873, 798988, 808579, 833105, 731004\n",
      "\n",
      " 436 x\n",
      "\tndos-direct_object => NP_direct_object\n",
      "\t650808, 698291, 736657, 852192, 606049, 606390, 606407, 606428, 606442, 606633\n",
      "\n",
      "  21 x\n",
      "\tlocs-p-predicate complement => location\n",
      "\t762650, 773835, 774139, 774162, 784987, 800764, 816576, 737240, 775557, 773644\n",
      "\n",
      "4202 x\n",
      "\tlocs-c-complement => location\n",
      "\t606803, 606854, 607721, 607724, 607780, 608092, 608166, 608238, 608280, 608470\n",
      "\n",
      " 450 x\n",
      "\tpdos-direct_object => principal_direct_object\n",
      "\t650807, 698290, 736658, 852191, 606048, 606389, 606406, 606427, 606441, 606632\n",
      "\n",
      "  7644 generic-phrase rule applications\n",
      "  8181 generic rule applications\n",
      "  1494 clause seen 1  time(s)\n",
      "     3 clause seen 2  time(s)\n",
      "  1497 clause seen in total\n",
      " 51763 phrase seen 1  time(s)\n",
      "   181 phrase seen 2  time(s)\n",
      "     9 phrase seen 3  time(s)\n",
      " 51953 phrase seen in total\n"
     ]
    }
   ],
   "source": [
    "seen = collections.defaultdict(collections.Counter)\n",
    "\n",
    "def gen_sheet_enrich(verb):\n",
    "    rows = []\n",
    "    fieldsep = ';'\n",
    "    clauses_seen = set()\n",
    "    for wn in occs[verb]:\n",
    "        cn = L.u('clause', wn)\n",
    "        if cn in clauses_seen: continue\n",
    "        clauses_seen.add(cn)\n",
    "        vn = L.u('verse', wn)\n",
    "        bn = L.u('book', wn)\n",
    "        book = T.book_name(bn, lang='en')\n",
    "        chapter = F.chapter.v(vn)\n",
    "        verse = F.verse.v(vn)\n",
    "        ln = ln_base+(ln_tpl.format(T.book_name(bn, lang='la'), chapter, verse))+ln_tweak\n",
    "        vl = F.lex.v(wn).rstrip('[=')\n",
    "        vstem = F.vs.v(wn)\n",
    "        vt = T.words([wn], fmt='ec').replace('\\n', '')\n",
    "        ct = T.words(L.d('word', cn), fmt='ec').replace('\\n', '')\n",
    "        \n",
    "        common_fields = (cn, wn, -1, book, chapter, verse, vl, vstem, vt, ct, '')\n",
    "        rows.append(common_fields + (('',)*fillrows))\n",
    "        for pn in L.d('phrase', cn):\n",
    "            seen['phrase'][pn] += 1\n",
    "            pt = T.words(L.d('word', pn), fmt='ec').replace('\\n', '')\n",
    "            common_fields = (cn, wn, pn, book, chapter, verse, vl, vstem, '', pt, 'phrase')\n",
    "            pty = F.typ.v(pn)\n",
    "            pf = pf_corr.get(pn, F.function.v(pn))\n",
    "            phrase_fields =\\\n",
    "                ('',)*pfillrows +\\\n",
    "                (pty, pf) +\\\n",
    "                apply_logic('phrase', vl, pn, transform['phrase'][pf])            \n",
    "            rows.append(common_fields + phrase_fields)\n",
    "        for scn in clause_objects[cn]:\n",
    "            seen['clause'][scn] += 1\n",
    "            sct = T.words(L.d('word', scn), fmt='ec').replace('\\n', '')\n",
    "            common_fields = (cn, wn, scn, book, chapter, verse, vl, vstem, '', sct, 'clause')\n",
    "            scty = F.typ.v(scn)\n",
    "            scr = F.rela.v(scn)\n",
    "            clause_fields =\\\n",
    "                (scty, scr) +\\\n",
    "                ('',)*cfillrows +\\\n",
    "                apply_logic('clause', vl, scn, transform['clause'][scr if scr == 'Objc' else scty])       \n",
    "            rows.append(common_fields + clause_fields)\n",
    "\n",
    "    filename = vfile(verb, 'enrich_blank')\n",
    "    row_file = open(filename, 'w')\n",
    "    row_file.write('{}\\n'.format(fieldsep.join(field_names)))\n",
    "    for row in rows:\n",
    "        row_file.write('{}\\n'.format(fieldsep.join(str(x) for x in row)))\n",
    "    row_file.close()\n",
    "    inf('Generated enrichment sheet for verb {} ({:>5} rows)'.format(verb, len(rows)), withtime=False)\n",
    "    \n",
    "for verb in verbs: gen_sheet_enrich(verb)\n",
    "\n",
    "inf('Done')\n",
    "for scope in rule_cases:\n",
    "    totalscope = 0\n",
    "    for kind in rule_cases[scope]:\n",
    "        inf('{}-{} rules:'.format(scope, kind), withtime=False)\n",
    "        totalkind = 0\n",
    "        for rule_spec in rule_cases[scope][kind]:\n",
    "            cases = rule_cases[scope][kind][rule_spec]\n",
    "            n = len(cases)\n",
    "            totalscope += n\n",
    "            totalkind += n\n",
    "            if scope == 'generic':\n",
    "                inf('{:>4} x\\n\\t{}\\n\\t{}\\n'.format(\n",
    "                    n, rule_as_str[scope](*rule_spec), \n",
    "                    ', '.join(str(c) for c in cases[0:10]),\n",
    "                ), withtime=False)\n",
    "            else:                \n",
    "                inf('{:>4} x\\n\\t{}\\n\\t{}\\n'.format(\n",
    "                    n, rule_as_str[scope](*rule_spec),\n",
    "                    ', '.join(str(c) for c in cases[0:10]),\n",
    "                ), withtime=False)\n",
    "        inf('{:>6} {}-{} rule applications'.format(totalkind, scope, kind), withtime=False)\n",
    "    inf('{:>6} {} rule applications'.format(totalscope, scope), withtime=False)\n",
    "\n",
    "for kind in seen:\n",
    "    stats = collections.Counter()\n",
    "    for (node, times) in seen[kind].items(): stats[times] += 1\n",
    "    for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "        inf('{:>6} {} seen {:<2} time(s)'.format(n, kind, times), withtime=False)\n",
    "    inf('{:>6} {} seen in total'.format(len(seen[kind]), kind), withtime=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def showcase(n):\n",
    "    otype = F.otype.v(n)\n",
    "    att1 = pf_corr.get(n, F.function.v(n)) if otype == 'phrase' else F.rela.v(n)\n",
    "    att2 = F.typ.v(n)\n",
    "    inf('''{} ({}-{}) {}\\n{}'''.format(\n",
    "        otype, att1, att2,\n",
    "        T.words(L.d('word', n), fmt='ec'), \n",
    "        T.text(\n",
    "            book=F.book.v(L.u('book', n)), \n",
    "            chapter=int(F.chapter.v(L.u('chapter', n))),\n",
    "            verse=int(F.verse.v(L.u('verse', n))), \n",
    "            fmt='ec', lang='la',\n",
    "        ),\n",
    "    ), withtime=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phrase (PreC-NP) PQWDJ HLWJ LM#PXTM \n",
      "Numeri 26:57\tW>LH PQWDJ HLWJ LM#PXTM LGR#WN M#PXT HGR#NJ LQHT M#PXT HQHTJ LMRRJ M#PXT HMRRJ00\n",
      "\n",
      "clause (Adju-InfC) L<#WT \n",
      "Deuteronomium 9:18\tW>TNPL LPNJ JHWH KR>#NH >RB<JM JWM W>RB<JM LJLH LXM L> >KLTJ WMJM L> #TJTJ <L KL&XV>TKM >#R XV>TM L<#WT HR< B<JNJ JHWH LHK<JSW00\n",
      "\n",
      "clause (Adju-InfC) BLKTK \n",
      "Exodus 4:21\tWJ>MR JHWH >L&M#H BLKTK L#WB MYRJMH R>H KL&HMPTJM >#R&#MTJ BJDK W<#JTM LPNJ PR<H W>NJ >XZQ >T&LBW WL> J#LX >T&H<M00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "showcase(654844)\n",
    "showcase(445014)\n",
    "showcase(432952)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verb BW>: 2570 occurrences. He locales in Cmpl phrases: 157\n",
      "\t26118, 26127, 146447, 187920, 197138, 272406, 95257, 184350, 398368, 289826, 201253, 24616, 78897, 401459, 100410, 32829, 100413, 198208, 5698, 200258, 100938, 24653, 141902, 112207, 186960, 24658, 196690, 28764, 34400, 298594, 248931, 132198, 162918, 12402, 5747, 146044, 396927, 153216, 134792, 151176, 188042, 97419, 426120, 257165, 136338, 21656, 162970, 200349, 214687, 24740, 257192, 158378, 100527, 25777, 160434, 214707, 4789, 4793, 272569, 139963, 90812, 249020, 38595, 113861, 138448, 8920, 282841, 19166, 20703, 26850, 43235, 145127, 8424, 8937, 170729, 397032, 254703, 154354, 200948, 426230, 176376, 79609, 165626, 206075, 208636, 27391, 269569, 106246, 157447, 26380, 149785, 170782, 211232, 126758, 26414, 27438, 246062, 109363, 172340, 249140, 398134, 64828, 26431, 16704, 4929, 168771, 154964, 132955, 393569, 47460, 157541, 47466, 100206, 37232, 269170, 23415, 410999, 23933, 24448, 78208, 133518, 25999, 191381, 12698, 19355, 24476, 170909, 18344, 157608, 267689, 244660, 256952, 8633, 63419, 167359, 175553, 138694, 110536, 175561, 108490, 111051, 143820, 37324, 192973, 264137, 5586, 99795, 11732, 170963, 20438, 218583, 269285, 25062, 110576, 26099, 184315, 256511\n",
      "Verb BW>: 2570 occurrences. He locales in Loca phrases: 14\n",
      "\t90243, 93571, 29637, 284965, 289859, 136745, 257293, 289871, 154354, 154964, 9525, 257016, 284989, 93598\n",
      "Verb BW>: 2570 occurrences. He locales in Adju phrases: 4\n",
      "\t154354, 322818, 154964, 75702\n"
     ]
    }
   ],
   "source": [
    "def check_h(vl, show_results=False):\n",
    "    hl = {}\n",
    "    total = 0\n",
    "    for w in F.otype.s('word'):\n",
    "        if F.sp.v(w) != 'verb' or F.lex.v(w).rstrip('[=/') != vl: continue\n",
    "        total += 1\n",
    "        c = L.u('clause', w)\n",
    "        ps = L.d('phrase', c)\n",
    "        phs = {p for p in ps if len({w for w in L.d('word', p) if F.uvf.v(w) == 'H'}) > 0}\n",
    "        for f in ('Cmpl', 'Adju', 'Loca'):\n",
    "            phc = {p for p in ps if pf_corr.get(p, None) or (pf_corr.get(p, F.function.v(p))) == f}\n",
    "            if len(phc & phs): hl.setdefault(f, set()).add(w)\n",
    "    for f in hl:\n",
    "        inf('Verb {}: {} occurrences. He locales in {} phrases: {}'.format(vl, total, f, len(hl[f])), withtime=False)\n",
    "        if show_results: inf('\\t{}'.format(', '.join(str(x) for x in hl[f])), withtime=False)\n",
    "check_h('BW>', show_results=True)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be handy to generate an informational spreadsheet that shows all these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Process the enrichments\n",
    "\n",
    "We read the enrichments, perform some consistency checks, and produce an annotation package.\n",
    "If the filled-in sheet does not exist, we take the blank sheet, with the default assignment of the new features.\n",
    "If a phrase got conflicting features, because it occurs in sheets for multiple verbs, the values in the filled-in sheet take precedence over the values in the blank sheet. If both occur in a filled in sheet, a warning will be issued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    17s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/oBR_etcbc4b.csv\n",
      "    17s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/oFH_etcbc4b.csv\n",
      "    17s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/oLH_etcbc4b.csv\n",
      "    17s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/BRa_etcbc4b.csv\n",
      "    17s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/BWa_etcbc4b.csv\n",
      "    17s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/CJT_etcbc4b.csv\n",
      "    17s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/CWB_etcbc4b.csv\n",
      "    18s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/HLK_etcbc4b.csv\n",
      "    18s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/JRD_etcbc4b.csv\n",
      "    18s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/JYa_etcbc4b.csv\n",
      "    18s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/NFa_etcbc4b.csv\n",
      "    18s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/NPL_etcbc4b.csv\n",
      "    18s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/NTN_etcbc4b.csv\n",
      "    18s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/NWS_etcbc4b.csv\n",
      "    18s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/PQD_etcbc4b.csv\n",
      "    18s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/QRa_etcbc4b.csv\n",
      "    18s NO filled enrichments file /Users/dirk/Dropbox/SYNVAR/enrich_filled/SWR_etcbc4b.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    18s OK: The used blank enrichment sheets have legal values\n",
      "    18s OK: The used blank enrichment sheets are consistent\n",
      "    18s OK: The used filled enrichment sheets have legal values\n",
      "    18s OK: The used filled enrichment sheets are consistent\n",
      "    18s OK: all enriched nodes where phrase nodes\n",
      "    18s OK: all enriched nodes occurred in the blank sheet\n",
      "    18s OK: there are 1878 manual correction/enrichment annotations\n",
      "COR                                    => Genesis 2:18 605699 adjunct,NA,NA,,,\n",
      "\tL.OW \n",
      "\t\t>E<:EFEH.&L.OW <;ZER K.:NEG:D.OW00\n",
      "\n",
      "COR                                    => Genesis 3:7 605887 adjunct,NA,NA,,,\n",
      "\tL@HEm \n",
      "\t\tWAJ.A<:AFW. L@HEm X:AGOROT00\n",
      "\n",
      "COR                                    => Genesis 3:21 606057 adjunct,NA,NA,,,\n",
      "\tL:>@D@m W.L:>IC:T.OW \n",
      "\t\tWAJ.A<AF J:HW@H >:ELOHIJm L:>@D@m W.L:>IC:T.OW K.@T:NOWT <OWR \n",
      "COR                                    => Genesis 6:14 606814 adjunct,NA,NA,,,\n",
      "\tL:k@ \n",
      "\t\t<:AF;H L:k@ T.;BAT <:AY;J&GOPER \n",
      "COR                                    => Genesis 6:16 606838 adjunct,NA,NA,,,\n",
      "\tLAT.;B@H \n",
      "\t\tYOHAR 05 T.A<:AFEH LAT.;B@H \n",
      "COR                                    => Genesis 9:24 607585 adjunct,NA,NA,,,\n",
      "\tLOW \n",
      "\t\t>;T >:ACER&<@F@H&LOW B.:NOW HAQ.@V@n00\n",
      "\n",
      "COR                                    => Genesis 11:4 607843 adjunct,NA,NA,,,\n",
      "\tL.@NW. \n",
      "\t\tW:NA<:AFEH&L.@NW. C;m \n",
      "COR                                    => Genesis 12:18 608323 adjunct,NA,NA,,,\n",
      "\tL.IJ \n",
      "\t\tMAH&Z.O>T <@FIJT@ L.IJ \n",
      "COR                                    => Genesis 14:2 608565 adjunct,NA,NA,,,\n",
      "\t>ET&B.ERA< MELEk: S:DOm W:>ET&B.IR:CA< MELEk: <:AMOR@H CIN:>@B 05 MELEk: >AD:M@H W:CEM:>;BER MELEk: Y:BOWJIm W.MELEk: B.ELA< \n",
      "\t\t<@FW. MIL:X@M@H >ET&B.ERA< MELEk: S:DOm W:>ET&B.IR:CA< MELEk: <:AMOR@H CIN:>@B 05 MELEk: >AD:M@H W:CEM:>;BER MELEk: Y:BOWJIm W.MELEk: B.ELA< \n",
      "COR                                    => Genesis 16:6 609084 adjunct,NA,NA,,,\n",
      "\tL@H. \n",
      "\t\t<:AFIJ&L@H. \n",
      "... AND 1868 ANNOTATIONS MORE\n",
      "    19s 41205  phrases seen 1  time(s)\n",
      "    19s 94     phrases seen 2  time(s)\n",
      "    19s 3      phrases seen 3  time(s)\n",
      "    19s Total phrases seen: 41302\n"
     ]
    }
   ],
   "source": [
    "objects_seen = collections.defaultdict(collections.Counter)\n",
    "\n",
    "def read_enrich(rootdir): # rootdir will not be used, data is computed from sheets\n",
    "    of_enriched = {\n",
    "        False: {}, # for enrichments found in blank sheets\n",
    "        True: {}, # for enrichments found in filled sheets\n",
    "    }\n",
    "    repeated = {\n",
    "        False: collections.defaultdict(list), # for blank sheets\n",
    "        True: collections.defaultdict(list), # for filled sheets\n",
    "    }\n",
    "    wrong_value = {\n",
    "        False: collections.defaultdict(list),\n",
    "        True: collections.defaultdict(list),\n",
    "    }\n",
    "\n",
    "    non_match = collections.defaultdict(list)\n",
    "    wrong_node = collections.defaultdict(list)\n",
    "\n",
    "    results = []\n",
    "    dev_results = [] # results that deviate from the filled sheet\n",
    "    \n",
    "    ERR_LIMIT = 10\n",
    "\n",
    "    for verb in sorted(verbs):\n",
    "        vresults = {\n",
    "            False: {}, # for blank sheets\n",
    "            True: {}, # for filled sheets\n",
    "        }\n",
    "        for check in (\n",
    "            (False, 'blank'), \n",
    "            (True, 'filled'),\n",
    "        ):\n",
    "            is_filled = check[0]\n",
    "            filename = vfile(verb, 'enrich_{}'.format(check[1]))\n",
    "            if not os.path.exists(filename):\n",
    "                msg('NO {} enrichments file {}'.format(check[1], filename))\n",
    "                continue\n",
    "            #inf('READING {} enrichments file {}'.format(check[1], filename))\n",
    "\n",
    "            with open(filename) as fh:\n",
    "                header = fh.__next__()\n",
    "                for line in fh:\n",
    "                    fields = line.rstrip().split(';')\n",
    "                    on = int(fields[2])\n",
    "                    if on < 0: continue\n",
    "                    kind = fields[10]\n",
    "                    objects_seen[kind][on] += 1\n",
    "                    vvals = tuple(fields[-nef:])\n",
    "                    for (f, v) in zip(enrich_fields, vvals):\n",
    "                        if v != '' and v != 'NA' and v not in enrich_fields[f]:\n",
    "                            wrong_value[is_filled][on].append((verb, f, v))\n",
    "                    vresults[is_filled][on] = vvals\n",
    "                    if on in of_enriched[is_filled]:\n",
    "                        if on not in repeated[is_filled]:\n",
    "                            repeated[is_filled][on] = [of_enriched[is_filled][on]]\n",
    "                        repeated[is_filled][on].append((verb, vvals))\n",
    "                    else:\n",
    "                        of_enriched[is_filled][on] = (verb, vvals)\n",
    "                    if F.otype.v(on) != kind: \n",
    "                        non_match[on].append((verb, kind))\n",
    "            for on in sorted(vresults[True]):          # check whether the phrase ids are not mangled\n",
    "                if on not in vresults[False]:\n",
    "                    wrong_node[on].append(verb)\n",
    "            for on in sorted(vresults[False]):      # now collect all results, give precedence to filled values\n",
    "                if F.otype.v(on) == 'phrase':\n",
    "                    f_corr = on in pf_corr  # manual correction in phrase function\n",
    "                    f_good = pf_corr.get(on, F.function.v(on)) \n",
    "                    s_manual = on in vresults[True] and vresults[False][on] != vresults[True][on] # real change\n",
    "                else:\n",
    "                    f_corr = ''\n",
    "                    f_good = ''\n",
    "                    s_manual = ''\n",
    "                these_results = vresults[True][on] if s_manual else vresults[False][on]\n",
    "                if f_corr or s_manual:\n",
    "                    dev_results.append((on,)+these_results+(f_good, f_corr, s_manual))\n",
    "                results.append((on,)+these_results+(f_good, f_corr, s_manual))\n",
    "\n",
    "    for check in (\n",
    "        (False, 'blank'), \n",
    "        (True, 'filled'),\n",
    "    ):\n",
    "        if len(wrong_value[check[0]]): #illegal values in sheets\n",
    "            wrongs = wrong_value[check[0]]\n",
    "            for x in sorted(wrongs)[0:ERR_LIMIT]:\n",
    "                px = T.words(L.d('word', x), fmt='ev')\n",
    "                ref_node = L.u('clause', x) if F.otype.v(x) != 'clause' else x\n",
    "                cx = T.words(L.d('word', ref_node), fmt='ev')\n",
    "                passage = T.passage(x)\n",
    "                msg('ERROR: {} Illegal value(s) in {}: {} = {} in {}:'.format(\n",
    "                    passage, check[1], x, px, cx\n",
    "                ), withtime=False)\n",
    "                for (verb, f, v) in wrongs[x]:\n",
    "                    msg('\\t\"{}\" is an illegal value for \"{}\" in verb {}'.format(\n",
    "                        v, f, verb,\n",
    "                    ), withtime=False)\n",
    "            ne = len(wrongs)\n",
    "            if ne > ERR_LIMIT: msg('... AND {} CASES MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "        else:\n",
    "            inf('OK: The used {} enrichment sheets have legal values'.format(check[1]))\n",
    "\n",
    "        nerrors = 0\n",
    "        if len(repeated[check[0]]): # duplicates in sheets, check consistency\n",
    "            repeats = repeated[check[0]]\n",
    "            for x in sorted(repeats):\n",
    "                overview = collections.defaultdict(list)\n",
    "                for y in repeats[x]: overview[y[1]].append(y[0])\n",
    "                px = T.words(L.d('word', x), fmt='ev')\n",
    "                ref_node = L.u('clause', x) if F.otype.v(x) != 'clause' else x\n",
    "                cx = T.words(L.d('word', ref_node), fmt='ev')\n",
    "                passage = T.passage(x)\n",
    "                if len(overview) > 1:\n",
    "                    nerrors += 1\n",
    "                    if nerrors < ERR_LIMIT:\n",
    "                        msg('ERROR: {} Conflict in {}: {} = {} in {}:'.format(\n",
    "                            passage, check[1], x, px, cx\n",
    "                        ), withtime=False)\n",
    "                        for vals in overview:\n",
    "                            msg('\\t{:<40} in verb(s) {}'.format(\n",
    "                                ', '.join(vals),\n",
    "                                ', '.join(overview[vals]),\n",
    "                        ), withtime=False)\n",
    "                elif False: # for debugging purposes\n",
    "                #else:\n",
    "                    nerrors += 1\n",
    "                    if nerrors < ERR_LIMIT:\n",
    "                        inf('{} Agreement in {} {} = {} in {}: {}'.format(\n",
    "                            passage, check[1], x, px, cx, ','.join(list(overview.values())[0]),\n",
    "                        ), withtime=False)\n",
    "            ne = nerrors\n",
    "            if ne > ERR_LIMIT: msg('... AND {} CASES MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "        if nerrors == 0:\n",
    "            inf('OK: The used {} enrichment sheets are consistent'.format(check[1]))\n",
    "\n",
    "    if len(non_match):\n",
    "        msg('ERROR: Enrichments have been applied to nodes with non-matching types:')\n",
    "        for x in sorted(non_match)[0:ERR_LIMIT]:\n",
    "            (verb, shouldbe) = non_match[x]\n",
    "            px = T.words(L.d('word', x), fmt='ev')\n",
    "            msg('{}: {} Node {} is not a {} but a {}'.format(\n",
    "                verb, T.passage(x), x, shouldbe, F.otype.v(x),\n",
    "            ), withtime=False)\n",
    "        ne = len(non_phrase)\n",
    "        if ne > ERR_LIMIT: msg('... AND {} CASES MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "    else:\n",
    "        inf('OK: all enriched nodes where phrase nodes')\n",
    "\n",
    "    if len(wrong_node):\n",
    "        msg('ERROR: Node in filled sheet did not occur in blank sheet:')\n",
    "        for x in sorted(wrong_node)[0:ERR_LIMIT]:\n",
    "            px = T.words(L.d('word', x), fmt='ev')\n",
    "            msg('{}: {} node {}'.format(\n",
    "                wrong_node[x], T.passage(x), x,\n",
    "            ), withtime=False)\n",
    "        ne = len(wrong_node)\n",
    "        if ne > ERR_LIMIT: msg('... AND {} CASES MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "    else:\n",
    "        inf('OK: all enriched nodes occurred in the blank sheet')\n",
    "\n",
    "    if len(dev_results):\n",
    "        inf('OK: there are {} manual correction/enrichment annotations'.format(len(dev_results)))\n",
    "        for r in dev_results[0:ERR_LIMIT]:\n",
    "            (x, *vals, f_good, f_corr, s_manual) = r\n",
    "            px = T.words(L.d('word', x), fmt='ev')\n",
    "            cx = T.words(L.d('word', L.u('clause', x)), fmt='ev')\n",
    "            inf('{:<30} {:>7} => {:<3} {:<3} {}\\n\\t{}\\n\\t\\t{}'.format(\n",
    "                'COR' if f_corr else '',\n",
    "                'MAN' if s_manual else'',\n",
    "                T.passage(x), x, ','.join(vals), px, cx\n",
    "            ), withtime=False)\n",
    "        ne = len(dev_results)\n",
    "        if ne > ERR_LIMIT: inf('... AND {} ANNOTATIONS MORE'.format(ne - ERR_LIMIT), withtime=False)\n",
    "    else:\n",
    "        msg('WARNING: there are no manual correction/enrichment annotations')\n",
    "    return results\n",
    "\n",
    "corr = ExtraData(API)\n",
    "corr.deliver_annots(\n",
    "    'complements', \n",
    "    {'title': 'Verb complement enrichments', 'date': '2016-06'},\n",
    "    [\n",
    "        (None, 'complements', read_enrich, tuple(\n",
    "            ('JanetDyk', 'ft', fname) for fname in list(enrich_fields.keys())+['function', 'f_correction', 's_manual']\n",
    "        ))\n",
    "    ],\n",
    ")\n",
    "\n",
    "stats = collections.Counter()\n",
    "for (p, times) in phrases_seen.items(): stats[times] += 1\n",
    "for (times, n) in sorted(stats.items(), key=lambda y: (-y[1], y[0])):\n",
    "    inf('{:<6} phrases seen {:<2} time(s)'.format(n, times))\n",
    "inf('Total phrases seen: {}'.format(len(phrases_seen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Annox complements\n",
    "We load the new and modified features into the LAF-Fabric API, in the process of which they will be compiled.\n",
    "\n",
    "Note that we draw in the new annotations by specifying an *annox* called `complements` (the second argument of the `fabric.load` function).\n",
    "\n",
    "Then we turn that data into LAF annotations. Every enrichment is stored in new features, \n",
    "with names specified above in ``enrich_fields``, \n",
    "with label `ft` and namespace `JanetDyk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s DETAIL: COMPILING m: etcbc4b: UP TO DATE\n",
      "  0.00s USING main: etcbc4b DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.00s BEGIN COMPILE a: complements\n",
      "  0.00s DETAIL: load main: X. [node]  -> \n",
      "  1.16s DETAIL: load main: X. [e]  -> \n",
      "  2.87s DETAIL: load main: G.node_anchor_min\n",
      "  2.92s DETAIL: load main: G.node_anchor_max\n",
      "  2.96s DETAIL: load main: G.node_sort\n",
      "  3.00s DETAIL: load main: G.node_sort_inv\n",
      "  3.39s DETAIL: load main: G.edges_from\n",
      "  3.45s DETAIL: load main: G.edges_to\n",
      "  3.50s LOGFILE=/Users/dirk/laf/laf-fabric-data/etcbc4b/bin/A/complements/__log__compile__.txt\n",
      "  3.50s PARSING ANNOTATION FILES\n",
      "  3.52s INFO: parsing complements.xml\n",
      "  6.90s INFO: END PARSING\n",
      "         0 good   regions  and     0 faulty ones\n",
      "         0 linked nodes    and     0 unlinked ones\n",
      "         0 good   edges    and     0 faulty ones\n",
      "     53450 good   annots   and     0 faulty ones\n",
      "    481050 good   features and     0 faulty ones\n",
      "     53450 distinct xml identifiers\n",
      "\n",
      "  6.90s MODELING RESULT FILES\n",
      "  6.90s INFO: CONNECTIVITY\n",
      "  7.05s WRITING RESULT FILES for a: complements\n",
      "  7.05s DETAIL: write annox complements: F.JanetDyk_ft_f_correction [node] \n",
      "  7.09s DETAIL: write annox complements: F.JanetDyk_ft_function [node] \n",
      "  7.13s DETAIL: write annox complements: F.JanetDyk_ft_grammatical [node] \n",
      "  7.16s DETAIL: write annox complements: F.JanetDyk_ft_lexical [node] \n",
      "  7.19s DETAIL: write annox complements: F.JanetDyk_ft_original [node] \n",
      "  7.21s DETAIL: write annox complements: F.JanetDyk_ft_predication [node] \n",
      "  7.24s DETAIL: write annox complements: F.JanetDyk_ft_s_manual [node] \n",
      "  7.28s DETAIL: write annox complements: F.JanetDyk_ft_semantic [node] \n",
      "  7.30s DETAIL: write annox complements: F.JanetDyk_ft_valence [node] \n",
      "  7.34s END   COMPILE a: complements\n",
      "  7.34s USING annox: complements DATA COMPILED AT: 2016-11-14T14-00-31\n",
      "  7.35s DETAIL: keep main: G.node_anchor_min\n",
      "  7.35s DETAIL: keep main: G.node_anchor_max\n",
      "  7.35s DETAIL: keep main: G.node_sort\n",
      "  7.35s DETAIL: keep main: G.node_sort_inv\n",
      "  7.35s DETAIL: keep main: G.edges_from\n",
      "  7.35s DETAIL: keep main: G.edges_to\n",
      "  7.35s DETAIL: keep main: F.etcbc4_db_otype [node] \n",
      "  7.35s DETAIL: keep main: F.etcbc4_ft_lex [node] \n",
      "  7.35s DETAIL: keep main: F.etcbc4_sft_chapter [node] \n",
      "  7.35s DETAIL: keep main: F.etcbc4_sft_verse [node] \n",
      "  7.35s DETAIL: clear main: F.etcbc4_ft_g_lex [node] \n",
      "  7.35s DETAIL: clear main: F.etcbc4_ft_g_lex_utf8 [node] \n",
      "  7.35s DETAIL: clear main: F.etcbc4_ft_g_word [node] \n",
      "  7.35s DETAIL: clear main: F.etcbc4_ft_g_word_utf8 [node] \n",
      "  7.36s DETAIL: clear main: F.etcbc4_ft_lex_utf8 [node] \n",
      "  7.36s DETAIL: clear main: F.etcbc4_ft_trailer_utf8 [node] \n",
      "  7.36s DETAIL: clear main: F.etcbc4_sft_book [node] \n",
      "  7.36s DETAIL: clear annox lexicon: F.etcbc4_kq_g_qere_utf8 [node] \n",
      "  7.36s DETAIL: clear annox lexicon: F.etcbc4_kq_qtrailer_utf8 [node] \n",
      "  7.36s DETAIL: clear annox lexicon: F.etcbc4_ph_phono [node] \n",
      "  7.38s DETAIL: clear annox lexicon: F.etcbc4_ph_phono_sep [node] \n",
      "  7.39s DETAIL: load main: F.etcbc4_db_oid [node] \n",
      "  8.14s DETAIL: load main: F.etcbc4_ft_function [node] \n",
      "  8.24s DETAIL: load main: F.etcbc4_ft_rela [node] \n",
      "  8.60s DETAIL: load main: F.etcbc4_ft_sp [node] \n",
      "  8.80s DETAIL: load main: F.etcbc4_ft_typ [node] \n",
      "  9.19s DETAIL: load main: F.etcbc4_ft_vs [node] \n",
      "  9.42s DETAIL: load annox complements: F.JanetDyk_ft_f_correction [node] \n",
      "  9.46s DETAIL: load annox complements: F.JanetDyk_ft_function [node] \n",
      "  9.50s DETAIL: load annox complements: F.JanetDyk_ft_grammatical [node] \n",
      "  9.53s DETAIL: load annox complements: F.JanetDyk_ft_lexical [node] \n",
      "  9.55s DETAIL: load annox complements: F.JanetDyk_ft_original [node] \n",
      "  9.58s DETAIL: load annox complements: F.JanetDyk_ft_predication [node] \n",
      "  9.61s DETAIL: load annox complements: F.JanetDyk_ft_s_manual [node] \n",
      "  9.64s DETAIL: load annox complements: F.JanetDyk_ft_semantic [node] \n",
      "  9.66s DETAIL: load annox complements: F.JanetDyk_ft_valence [node] \n",
      "  9.69s INFO: Feature ft_function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  9.69s INFO: Feature function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "  9.73s INFO: LOADING PREPARED data: please wait ... \n",
      "  9.73s DETAIL: keep prep: G.node_sort\n",
      "  9.74s DETAIL: keep prep: G.node_sort_inv\n",
      "  9.74s DETAIL: keep prep: L.node_up\n",
      "  9.74s DETAIL: keep prep: L.node_down\n",
      "  9.74s DETAIL: keep prep: V.verses\n",
      "  9.74s DETAIL: keep prep: V.books_la\n",
      "  9.74s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "    10s INFO: Feature ft_function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "    10s INFO: Feature function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "    12s INFO: Feature ft_function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "    12s INFO: Feature function refers to etcbc4_ft_function, not to JanetDyk_ft_function\n",
      "    12s INFO: LOADED PREPARED data\n",
      "    12s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX complements, lexicon FOR TASK flow_corr AT 2016-11-14T14-00-36\n"
     ]
    }
   ],
   "source": [
    "API=fabric.load(source+version, 'complements', 'flow_corr', {\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype\n",
    "        sp vs lex\n",
    "        rela typ\n",
    "        chapter verse\n",
    "        etcbc4:ft.function JanetDyk:ft.function\n",
    "        s_manual f_correction\n",
    "    ''' + ' '.join(enrich_fields),\n",
    "    '''\n",
    "    '''),\n",
    "    \"prepare\": prepare,\n",
    "}, verbose='DETAIL')\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple test\n",
    "Take the first 10 phrases and retrieve the corrected and uncorrected function feature.\n",
    "Note that the corrected function feature is only filled in, if it occurs in a clause in which a selected verb occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time - Time - True\n",
      "Pred - Pred - True\n",
      "Subj - Subj - True\n",
      "Objc - Objc - True\n",
      "Conj - None - False\n",
      "Subj - None - False\n",
      "Pred - None - False\n",
      "PreC - None - False\n",
      "Conj - None - False\n",
      "Subj - None - False\n"
     ]
    }
   ],
   "source": [
    "for i in list(F.otype.s('phrase'))[0:10]: \n",
    "    print('{} - {} - {}'.format(\n",
    "        F.function.v(i), \n",
    "        F.JanetDyk_ft_function.v(i),\n",
    "        L.u('clause', i) in clause_verb,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "We put all corrections and enrichments in a single csv file for checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.64s collecting constituents ...\n",
      "  2.49s  10000 constituents in  2823 clauses ...\n",
      "  3.32s  20000 constituents in  5707 clauses ...\n",
      "  4.10s  30000 constituents in  8713 clauses ...\n",
      "  4.93s  40000 constituents in 11768 clauses ...\n",
      "  5.75s  50000 constituents in 14914 clauses ...\n",
      "  6.05s  53450 constituents in 15880 clauses done\n"
     ]
    }
   ],
   "source": [
    "f = open(all_results, 'w')\n",
    "NALLFIELDS = 17\n",
    "tpl = ('{};' * (NALLFIELDS - 1))+'{}\\n'\n",
    "\n",
    "inf('collecting constituents ...')\n",
    "f.write(tpl.format(\n",
    "    '-',\n",
    "    '-',\n",
    "    'passage',\n",
    "    'verb(s) text',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    '-',\n",
    "    'clause text',\n",
    "    'clause node',\n",
    "))\n",
    "f.write(tpl.format(\n",
    "    'corrected',\n",
    "    'enriched',\n",
    "    'passage',\n",
    "    '-',\n",
    "    'object type',\n",
    "    'clause rela',\n",
    "    'clause type',\n",
    "    'phrase function (old)',\n",
    "    'phrase function (new)',\n",
    "    'phrase type',\n",
    "    'valence',\n",
    "    'predication',\n",
    "    'grammatical',\n",
    "    'original',\n",
    "    'lexical',\n",
    "    'semantic',\n",
    "    'object text',\n",
    "    'object node',\n",
    "))\n",
    "i = 0\n",
    "j = 0\n",
    "c = 0\n",
    "CHUNK_SIZE = 10000\n",
    "for cn in sorted(clause_verb):\n",
    "    c += 1\n",
    "    vrbs = sorted(clause_verb[cn])\n",
    "    f.write(tpl.format(\n",
    "        '',\n",
    "        '',\n",
    "        T.passage(cn),\n",
    "        ' '.join(F.lex.v(verb) for verb in vrbs),\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        '',\n",
    "        T.words(L.d('word', cn), fmt='ec').replace('\\n', ' '),\n",
    "        cn,\n",
    "    ))\n",
    "    for pn in L.d('phrase', cn):\n",
    "        i += 1\n",
    "        j += 1\n",
    "        if j == CHUNK_SIZE:\n",
    "            j = 0\n",
    "            inf('{:>6} constituents in {:>5} clauses ...'.format(i, c))\n",
    "        f.write(tpl.format(\n",
    "            'COR' if F.f_correction.v(pn) == 'True' else '',\n",
    "            'MAN' if F.s_manual.v(pn) == 'True' else '',\n",
    "            T.passage(pn),\n",
    "            '',\n",
    "            'phrase',\n",
    "            '',\n",
    "            '',\n",
    "            F.etcbc4_ft_function.v(pn),\n",
    "            F.JanetDyk_ft_function.v(pn),\n",
    "            F.typ.v(pn),\n",
    "            F.valence.v(pn),\n",
    "            F.predication.v(pn),\n",
    "            F.grammatical.v(pn),\n",
    "            F.original.v(pn),\n",
    "            F.lexical.v(pn),\n",
    "            F.semantic.v(pn),\n",
    "            T.words(L.d('word', pn), fmt='ec').replace('\\n', ' '),\n",
    "            pn,\n",
    "        ))\n",
    "    for scn in clause_objects[cn]:\n",
    "        i += 1\n",
    "        j += 1\n",
    "        if j == CHUNK_SIZE:\n",
    "            j = 0\n",
    "            inf('{:>6} constituents in {:>5} clauses ...'.format(i, c))\n",
    "        f.write(tpl.format(\n",
    "            '',\n",
    "            '',\n",
    "            T.passage(scn),\n",
    "            '',\n",
    "            'clause',\n",
    "            F.rela.v(scn),\n",
    "            F.typ.v(scn),\n",
    "            '',\n",
    "            '',\n",
    "            '',\n",
    "            F.valence.v(scn),\n",
    "            F.predication.v(scn),\n",
    "            F.grammatical.v(scn),\n",
    "            F.original.v(scn),\n",
    "            F.lexical.v(scn),\n",
    "            F.semantic.v(scn),\n",
    "            T.words(L.d('word', scn), fmt='ec').replace('\\n', ' '),\n",
    "            scn,\n",
    "        ))\n",
    "\n",
    "f.close()\n",
    "inf('{:>6} constituents in {:>5} clauses done'.format(i, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
