{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://laf-fabric.readthedocs.org/en/latest/\" target=\"_blank\"><img align=\"left\" src=\"images/laf-fabric-xsmall.png\"/></a>\n",
    "<a href=\"http://www.godgeleerdheid.vu.nl/etcbc\" target=\"_blank\"><img align=\"left\" src=\"images/VU-ETCBC-xsmall.png\"/></a>\n",
    "<a href=\"http://www.persistent-identifier.nl/?identifier=urn%3Anbn%3Anl%3Aui%3A13-048i-71\" target=\"_blank\"><img align=\"left\"src=\"images/etcbc4easy-small.png\"/></a>\n",
    "<a href=\"http://tla.mpi.nl\" target=\"_blank\"><img align=\"right\" src=\"images/TLA-xsmall.png\"/></a>\n",
    "<a href=\"http://www.dans.knaw.nl\" target=\"_blank\"><img align=\"right\"src=\"images/DANS-xsmall.png\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETCBC in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook exports the ETCBC database to an R data frame.\n",
    "The nodes are exported as rows, they correspond to the text objects such as word, phrase, clause, sentence, verse, chapter, book and a few others.\n",
    "\n",
    "The etcbc features become the columns, so each row tells what values the features have for the corresponding object.\n",
    "\n",
    "The edges corresponding to the etcbc features *mother*, *functional_parent*, *distributional_parent* are\n",
    "exported as extra columns. For each row, such a column indicates the target of a corresponding outgoing edge.\n",
    "In the ETCBC data objects have at most one outgoing edge for each type of edge. The target is identified by its object identifier, which is in the ``oid`` column.\n",
    "\n",
    "We also write the data that says which objects are contained in which.\n",
    "In the ETCBC data, an object is contained in at most one object of each object type.\n",
    "So a word is contained in exactly one phrase, never in more than one.\n",
    "Another way to say this is that objects of the same type never overlap.\n",
    "To each row we add the following columns:\n",
    "\n",
    "* for each object type, execpt `word` there is a column with name that object type and containing\n",
    "  the identifier of the containing object of that type of the row object (if any).\n",
    "\n",
    "Extra data such as lexicon (including frequency and rank features), phonetic transcription, and ketiv-qere are also included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is the file \n",
    "``etcbc4b.rds``\n",
    "in the \n",
    "[laf-fabric-data github repository](https://github.com/ETCBC/laf-fabric-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.5.7\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: https://shebanq.ancient-data.org/static/docs/featuredoc/texts/welcome.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, collections\n",
    "\n",
    "from laf.fabric import LafFabric\n",
    "import etcbc\n",
    "from etcbc.preprocess import prepare\n",
    "\n",
    "fabric = LafFabric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2016-01-27T18-59-08\n",
      "  1.18s LOGFILE=/Users/dirk/SURFdrive/laf-fabric-output/etcbc4b/hinr/__log__hinr.txt\n",
      "  1.18s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX para FOR TASK hinr AT 2016-01-29T13-41-45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('etcbc4', ['db.maxmonad', 'db.minmonad', 'db.monads', 'db.oid', 'db.otype', 'ft.code', 'ft.det', 'ft.dist', 'ft.dist_unit', 'ft.domain', 'ft.function', 'ft.g_cons', 'ft.g_cons_utf8', 'ft.g_lex', 'ft.g_lex_utf8', 'ft.g_nme', 'ft.g_nme_utf8', 'ft.g_pfm', 'ft.g_pfm_utf8', 'ft.g_prs', 'ft.g_prs_utf8', 'ft.g_uvf', 'ft.g_uvf_utf8', 'ft.g_vbe', 'ft.g_vbe_utf8', 'ft.g_vbs', 'ft.g_vbs_utf8', 'ft.g_word', 'ft.g_word_utf8', 'ft.gn', 'ft.is_root', 'ft.kind', 'ft.language', 'ft.lex', 'ft.lex_utf8', 'ft.ls', 'ft.mother_object_type', 'ft.nme', 'ft.nu', 'ft.number', 'ft.pdp', 'ft.pfm', 'ft.prs', 'ft.ps', 'ft.rela', 'ft.sp', 'ft.st', 'ft.tab', 'ft.trailer_utf8', 'ft.txt', 'ft.typ', 'ft.uvf', 'ft.vbe', 'ft.vbs', 'ft.vs', 'ft.vt', 'px.instruction', 'px.number_in_ch', 'px.pargr', 'sft.book', 'sft.chapter', 'sft.label', 'sft.verse'])]\n"
     ]
    }
   ],
   "source": [
    "API = fabric.load('etcbc4b', 'para', 'hinr', {\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "''',\"\"),\n",
    "    \"primary\": False,\n",
    "})\n",
    "print(API['F_all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'instruction number_in_ch pargr'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_features = [x.split('.', 1)[1] for x in API['F_all'][0][1] if x.split('.', 1)[0] == 'px']\n",
    "p_feature_str = ' '.join(p_features)\n",
    "p_feature_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2016-01-27T18-59-08\n",
      "  0.23s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX para FOR TASK hinr AT 2016-01-29T13-41-50\n"
     ]
    }
   ],
   "source": [
    "API = fabric.load_again({\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": (p_feature_str,\"\"),\n",
    "    \"primary\": False,\n",
    "})\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7.29s Collecting R data for paragraphs\n",
      "  7.73s  100000 nodes read\n",
      "  8.06s  200000 nodes read\n",
      "  8.39s  300000 nodes read\n",
      "  8.72s  400000 nodes read\n",
      "  9.04s  500000 nodes read\n",
      "  9.36s  600000 nodes read\n",
      "  9.72s  700000 nodes read\n",
      "    10s  800000 nodes read\n",
      "    11s  900000 nodes read\n",
      "    11s 1000000 nodes read\n",
      "    12s 1100000 nodes read\n",
      "    12s 1200000 nodes read\n",
      "    12s 1300000 nodes read\n",
      "    13s 1400000 nodes read\n",
      "    13s 1436858 nodes read and done\n"
     ]
    }
   ],
   "source": [
    "msg(\"Collecting R data for paragraphs\")\n",
    "chunk_size = 100000\n",
    "i = 0\n",
    "s = 0\n",
    "p_data = {}\n",
    "for n in NN():\n",
    "    p_values = []\n",
    "    p_data[n] = tuple(F.item[x].v(n) for x in p_features)\n",
    "    i += 1\n",
    "    s += 1\n",
    "    if s == chunk_size:\n",
    "        s = 0\n",
    "        msg('{:>7} nodes read'.format(i))\n",
    "msg('{:>7} nodes read and done'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2016-01-27T19-01-17\n",
      "  0.01s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK hinr AT 2016-01-29T13-42-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('etcbc4', ['db.maxmonad', 'db.minmonad', 'db.monads', 'db.oid', 'db.otype', 'ft.code', 'ft.det', 'ft.dist', 'ft.dist_unit', 'ft.domain', 'ft.function', 'ft.g_cons', 'ft.g_cons_utf8', 'ft.g_lex', 'ft.g_lex_utf8', 'ft.g_nme', 'ft.g_nme_utf8', 'ft.g_pfm', 'ft.g_pfm_utf8', 'ft.g_prs', 'ft.g_prs_utf8', 'ft.g_uvf', 'ft.g_uvf_utf8', 'ft.g_vbe', 'ft.g_vbe_utf8', 'ft.g_vbs', 'ft.g_vbs_utf8', 'ft.g_word', 'ft.g_word_utf8', 'ft.gn', 'ft.is_root', 'ft.kind', 'ft.language', 'ft.lex', 'ft.lex_utf8', 'ft.ls', 'ft.mother_object_type', 'ft.nme', 'ft.nu', 'ft.number', 'ft.pdp', 'ft.pfm', 'ft.prs', 'ft.ps', 'ft.rela', 'ft.sp', 'ft.st', 'ft.tab', 'ft.trailer_utf8', 'ft.txt', 'ft.typ', 'ft.uvf', 'ft.vbe', 'ft.vbs', 'ft.vs', 'ft.vt', 'kq.g_qere_utf8', 'kq.qtrailer_utf8', 'lex.entry', 'lex.entry_heb', 'lex.entryid', 'lex.freq_lex', 'lex.freq_occ', 'lex.g_entry', 'lex.g_entry_heb', 'lex.gloss', 'lex.id', 'lex.lan', 'lex.nametype', 'lex.pos', 'lex.rank_lex', 'lex.rank_occ', 'lex.root', 'lex.subpos', 'ph.phono', 'ph.phono_sep', 'sft.book', 'sft.chapter', 'sft.label', 'sft.verse'])]\n",
      "[('etcbc4', ['ft.distributional_parent', 'ft.functional_parent', 'ft.mother']), ('laf', [('', 'x'), ('', 'y')])]\n"
     ]
    }
   ],
   "source": [
    "API = fabric.load('etcbc4b', 'lexicon', 'hinr', {\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "''',\"\"),\n",
    "    \"primary\": False,\n",
    "})\n",
    "\n",
    "print(API['F_all'])\n",
    "print(API['FE_all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distributional_parent functional_parent mother'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features = [x.split('.', 1)[1] for x in API['F_all'][0][1]]\n",
    "all_feature_str = ' '.join(all_features)\n",
    "alle_features = [x.split('.', 1)[1] for x in API['FE_all'][0][1]]\n",
    "alle_feature_str = ' '.join(alle_features)\n",
    "alle_feature_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2016-01-27T19-01-17\n",
      "    44s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "  0.00s LOADING API with EXTRAs: please wait ... \n",
      "  0.00s INFO: USING DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.00s INFO: USING DATA COMPILED AT: 2016-01-27T19-01-17\n",
      "  0.01s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK hinr AT 2016-01-29T13-42-58\n",
      "  0.00s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK hinr AT 2016-01-29T13-42-58\n"
     ]
    }
   ],
   "source": [
    "API = fabric.load_again({\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": (all_feature_str,alle_feature_str),\n",
    "    \"primary\": False,\n",
    "    \"prepare\": prepare,\n",
    "})\n",
    "exec(fabric.localnames.format(var='fabric'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 9m 25s Writing R feature data\n",
      " 9m 34s  100000 nodes written\n",
      " 9m 45s  200000 nodes written\n",
      " 9m 54s  300000 nodes written\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-dd38215e5e83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#    l_values = [L.u(x, n) or '' for x in l_features]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     hr.write('{}\\t{}\\t{}\\n'.format(\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malle_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-dd38215e5e83>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#    l_values = [L.u(x, n) or '' for x in l_features]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     hr.write('{}\\t{}\\t{}\\n'.format(\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malle_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "msg(\"Writing R feature data\")\n",
    "hr = open('/Users/dirk/SURFdrive/laf-fabric-data/r/etcbc4b.txt', 'w')\n",
    "\n",
    "use_features = all_features\n",
    "usee_features = alle_features\n",
    "l_features = '''\n",
    "    subphrase phrase_atom phrase clause_atom clause sentence_atom sentence\n",
    "    half_verse verse chapter book\n",
    "'''.strip().split()\n",
    "\n",
    "\n",
    "hr.write('{}\\t{}\\t{}\\n'.format(\n",
    "    '\\t'.join(use_features),\n",
    "    '\\t'.join(p_features),\n",
    "    '\\t'.join(usee_features),\n",
    "    '\\t'.join(l_features),\n",
    "))\n",
    "chunk_size = 100000\n",
    "i = 0\n",
    "s = 0\n",
    "for n in NN():\n",
    "    all_values = [F.item[x].v(n) for x in use_features]\n",
    "    p_values = p_data[n]\n",
    "    alle_values = [F.oid.v((list(C.item[x].v(n)) or [-1])[0]) for x in usee_features]\n",
    "    l_values = [L.u(x, n) or '' for x in l_features]\n",
    "    hr.write('{}\\t{}\\t{}\\n'.format(\n",
    "        ('\\t'.join(x or '' for x in all_values)).replace('\\n',''),\n",
    "        ('\\t'.join(x or '' for x in p_values)).replace('\\n',''),\n",
    "        ('\\t'.join(x or '' for x in alle_values)),\n",
    "        ('\\t'.join(str(x) for x in l_values)),\n",
    "    ))\n",
    "    i += 1\n",
    "    s += 1\n",
    "    if s == chunk_size:\n",
    "        s = 0\n",
    "        msg('{:>7} nodes written'.format(i))\n",
    "hr.close()\n",
    "msg('{:>7} nodes written and done'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 886752\r\n",
      "-rw-r--r--  1 dirk  staff    49M Jan 29 11:00 etcbc4b.rds\r\n",
      "-rw-r--r--  1 dirk  staff   265M Jan 29 10:52 etcbc4b.txt\r\n",
      "-rw-r--r--  1 dirk  staff   6.6M Jan 29 13:38 etcbc4b_L.rds\r\n",
      "-rw-r--r--  1 dirk  staff   113M Jan 29 13:58 etcbc4b_L.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /Users/dirk/SURFdrive/laf-fabric-data/r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R export is ready now, but it is a bit large.\n",
    "We can get a much leaner file by using R to load this file and save it in .rds format.\n",
    "\n",
    "If your installation is not such that you can run R in a notebook based on a python kernel (as I am experiencing right now due to problems with the python module `rpy2`), switch to the Hebrew_In_RR notebook in this same directory to \n",
    "carry out the R cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy it to the github directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cp '/Users/dirk/SURFdrive/laf-fabric-data/r/etcbc4b.rds' '/Users/dirk/SURFdrive/current/demos/github/laf-fabric-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
